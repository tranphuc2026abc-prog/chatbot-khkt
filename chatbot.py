# Ch·∫°y b·∫±ng l·ªánh: streamlit run chatbot.py
# ‚ÄºÔ∏è Y√™u c·∫ßu c√†i ƒë·∫∑t: pip install google-generativeai streamlit pypdf scikit-learn
# (L∆∞u √Ω: Pypdf v√† Scikit-learn l√† B·∫ÆT BU·ªòC ƒë·ªÉ RAG ho·∫°t ƒë·ªông)

import streamlit as st
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import os
import glob
import time

# --- TH∆Ø VI·ªÜN B·∫ÆT BU·ªòC CHO RAG ---
from pypdf import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
# --- K·∫æT TH√öC TH∆Ø VI·ªÜN RAG ---


# --- B∆Ø·ªöC 1: L·∫§Y API KEY ---
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
except (KeyError, FileNotFoundError):
    st.error("L·ªói: Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY. Vui l√≤ng th√™m v√†o Secrets tr√™n Streamlit Cloud.")
    st.stop()
    
genai.configure(api_key=api_key)

# --- B∆Ø·ªöC 2: THI·∫æT L·∫¨P VAI TR√í (SYSTEM_INSTRUCTION) ---
SYSTEM_INSTRUCTION = """
---
B·ªêI C·∫¢NH VAI TR√í (ROLE CONTEXT)
---
B·∫°n l√† ‚ÄúChatbook‚Äù, m·ªôt C·ªë v·∫•n H·ªçc t·∫≠p Tin h·ªçc AI to√†n di·ªán.
Vai tr√≤ c·ªßa b·∫°n ƒë∆∞·ª£c m√¥ ph·ªèng theo m·ªôt **Gi√°o vi√™n Tin h·ªçc d·∫°y gi·ªèi c·∫•p Qu·ªëc gia**: t·∫≠n t√¢m, hi·ªÉu bi·∫øt s√¢u r·ªông, v√† lu√¥n ki√™n nh·∫´n.
M·ª•c ti√™u c·ªßa b·∫°n l√† ƒë·ªìng h√†nh, h·ªó tr·ª£ h·ªçc sinh THCS v√† THPT (t·ª´ l·ªõp 6 ƒë·∫øn l·ªõp 12) n·∫Øm v·ªØng ki·∫øn th·ª©c, ph√°t tri·ªÉn nƒÉng l·ª±c Tin h·ªçc theo **Chu·∫©n ch∆∞∆°ng tr√¨nh Gi√°o d·ª•c Ph·ªï th√¥ng 2018** c·ªßa Vi·ªát Nam.

---
üìö N·ªÄN T·∫¢NG TRI TH·ª®C C·ªêT L√ïI (CORE KNOWLEDGE BASE) - B·∫ÆT BU·ªòC
---
B·∫°n **PH·∫¢I** n·∫Øm v·ªØng v√† s·ª≠ d·ª•ng th√†nh th·∫°o to√†n b·ªô h·ªá th·ªëng ki·∫øn th·ª©c trong S√°ch gi√°o khoa Tin h·ªçc t·ª´ l·ªõp 6 ƒë·∫øn l·ªõp 12 c·ªßa **C·∫¢ BA B·ªò S√ÅCH HI·ªÜN H√ÄNH**:
1.  **K·∫øt n·ªëi tri th·ª©c v·ªõi cu·ªôc s·ªëng (KNTT)**
2.  **C√°nh Di·ªÅu (CD)**
3.  **Ch√¢n tr·ªùi s√°ng t·∫°o (CTST)**

#... (Gi·ªØ nguy√™n to√†n b·ªô d·ªØ li·ªáu m·ª•c l·ª•c v√† ph·∫ßn c√≤n l·∫°i c·ªßa System Prompt) ...

---
[PH·∫¶N QUAN TR·ªåNG] X·ª¨ L√ù TH√îNG TIN TRA C·ª®U (RAG)
---
Khi nh·∫≠n ƒë∆∞·ª£c th√¥ng tin trong m·ªôt tin nh·∫Øn h·ªá th·ªëng b·∫Øt ƒë·∫ßu b·∫±ng "--- B·∫ÆT ƒê·∫¶U D·ªÆ LI·ªÜU TRA C·ª®U T·ª™ 'S·ªî TAY' (RAG) ---", b·∫°n **PH·∫¢I** tu√¢n th·ªß c√°c quy t·∫Øc sau:

1.  **∆ØU TI√äN TUY·ªÜT ƒê·ªêI:** D·ªØ li·ªáu n√†y l√† ngu·ªìn "ch√¢n l√Ω" (ground truth) t·ª´ S·ªï tay Tin h·ªçc. B·∫°n **PH·∫¢I** ∆∞u ti√™n s·ª≠ d·ª•ng th√¥ng tin n√†y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
2.  **TR√çCH D·∫™N (N·∫æU C√ì TH·ªÇ):** N·∫øu c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n d·ª±a tr·ª±c ti·∫øp v√†o "NGU·ªíN" ƒë∆∞·ª£c cung c·∫•p, h√£y c·ªë g·∫Øng tr√≠ch d·∫´n ng·∫Øn g·ªçn (v√≠ d·ª•: "Theo t√†i li·ªáu,..." ho·∫∑c "Nh∆∞ trong S·ªï tay c√≥ ƒë·ªÅ c·∫≠p...").
3.  **T·ªîNG H·ª¢P:** N·∫øu c√°c NGU·ªíN cung c·∫•p th√¥ng tin r·ªùi r·∫°c, h√£y t·ªïng h·ª£p ch√∫ng l·∫°i th√†nh m·ªôt c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c.
4.  **KH√îNG B·ªäA ƒê·∫∂T:** N·∫øu th√¥ng tin tra c·ª©u c√≥ v·∫ª kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi, h√£y l·ªãch s·ª± th√¥ng b√°o r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c trong S·ªï tay v√† tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa b·∫°n.
"""

# --- B∆Ø·ªöC 3: KH·ªûI T·∫†O CLIENT V√Ä CH·ªåN M√î H√åNH ---

# [S·ª¨A L·ªñI] D√πng 'gemini-pro' (c∆° b·∫£n) ƒë·ªÉ ƒë·∫£m b·∫£o API Key c√≥ quy·ªÅn
MODEL_NAME = 'gemini-pro' 
try:
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }
    
    gemini_model = genai.GenerativeModel(
        model_name=MODEL_NAME,
        system_instruction=SYSTEM_INSTRUCTION,
        safety_settings=safety_settings
    )
    print("Kh·ªüi t·∫°o model Gemini th√†nh c√¥ng.") # DEBUG
except Exception as e:
    st.error(f"L·ªói khi kh·ªüi t·∫°o Model Gemini: {e}")
    st.stop()


# --- B∆Ø·ªöC 4: C·∫§U H√åNH TRANG V√Ä CSS ---
# (Gi·ªØ nguy√™n kh√¥ng thay ƒë·ªïi)
st.set_page_config(page_title="Chatbot Tin h·ªçc 2018", page_icon="‚ú®", layout="centered")
st.markdown("""
<style>
    /* ... (To√†n b·ªô CSS c·ªßa th·∫ßy gi·ªØ nguy√™n) ... */
    #MainMenu {visibility: hidden;} footer {visibility: hidden;} header {visibility: hidden;}
    [data-testid="stSidebar"] {
        background-color: #f8f9fa; border-right: 1px solid #e6e6e6;
    }
    .main .block-container { 
        max-width: 850px; padding-top: 2rem; padding-bottom: 5rem;
    }
    .welcome-message { font-size: 1.1em; color: #333; }
</style>
""", unsafe_allow_html=True)


# --- B∆Ø·ªöC 4.5: THANH B√äN (SIDEBAR) ---
with st.sidebar:
    st.title("ü§ñ Chatbot KTC")
    st.markdown("---")
    
    if st.button("‚ûï Cu·ªôc tr√≤ chuy·ªán m·ªõi", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.markdown("---")
    st.markdown(
        "Gi√°o vi√™n h∆∞·ªõng d·∫´n:\n"
        "**Th·∫ßy Nguy·ªÖn Th·∫ø Khanh** (GV Tin h·ªçc)\n\n"
        "H·ªçc sinh th·ª±c hi·ªán:\n"
        "*(B√πi T√° T√πng)*\n"
        "*(Cao S·ªπ B·∫£o Chung)*"
    )
    st.markdown("---")
    st.caption(f"Model: {MODEL_NAME}")


# --- B∆Ø·ªöC 4.6: C√ÅC H√ÄM RAG ---
# (ƒê·ªÉ c√°c h√†m n√†y ·ªü ƒë√¢y, ch√∫ng ta ch·ªâ kh√¥ng g·ªçi h√†m load_and_process_pdfs)

@st.cache_data(ttl=3600) 
def load_and_process_pdfs(pdf_folder="data_pdf"):
    print(f"--- B·∫ÆT ƒê·∫¶U H√ÄM load_and_process_pdfs ---") # DEBUG
    print(f"B·∫Øt ƒë·∫ßu qu√©t th∆∞ m·ª•c: {pdf_folder}")
    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    
    if not pdf_files:
        print("C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y file PDF n√†o.")
        return None 

    chunks = []
    for pdf_path in pdf_files:
        print(f"ƒêang x·ª≠ l√Ω file: {pdf_path}")
        try:
            reader = PdfReader(pdf_path)
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                if text:
                    source_info = f"[Ngu·ªìn: {os.path.basename(pdf_path)}, Trang {page_num + 1}]"
                    chunks.append(f"{source_info}\n\n{text}")
        except Exception as e:
            st.error(f"L·ªói khi ƒë·ªçc file {pdf_path}: {e}. Vui l√≤ng ki·ªÉm tra file n√†y tr√™n GitHub.")
            print(f"L·ªói khi ƒë·ªçc file {pdf_path}: {e}") # DEBUG
            continue 

    if not chunks:
        print("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ c√°c file PDF.")
        return None 

    print(f"ƒê√£ tr√≠ch xu·∫•t {len(chunks)} trang PDF. B·∫Øt ƒë·∫ßu vector h√≥a...")
    
    try:
        vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform(chunks)
        print("Vector h√≥a ho√†n t·∫•t.")
        return (chunks, tfidf_matrix, vectorizer)
    
    except ValueError as e:
        if "empty vocabulary" in str(e):
            st.error(f"L·ªói RAG: C√°c file PDF c√≥ th·ªÉ kh√¥ng ch·ª©a vƒÉn b·∫£n (ch·ªâ ch·ª©a ·∫£nh).")
        else:
            st.error(f"L·ªói Vectorizer: {e}")
        return None 
    

def find_relevant_knowledge(query, knowledge_data, num_chunks=3):
    chunks, tfidf_matrix, vectorizer = knowledge_data
    
    if not chunks or tfidf_matrix is None or vectorizer is None:
        return [] 

    query_vector = vectorizer.transform([query])
    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
    relevant_indices = np.where(cosine_similarities > 0.1)[0]
    sorted_indices = sorted(relevant_indices, key=lambda i: cosine_similarities[i], reverse=True)
    top_indices = sorted_indices[:num_chunks]

    if not top_indices:
        return [] 
        
    relevant_chunks = [chunks[i] for i in top_indices]
    return relevant_chunks

def convert_history_for_gemini(messages):
    gemini_history = []
    for msg in messages:
        role = 'model' if msg['role'] == 'assistant' else 'user'
        gemini_history.append({'role': role, 'parts': [msg['content']]})
    return gemini_history

# --- [S·ª¨A L·ªñI THEO Y√äU C·∫¶U] B∆Ø·ªöC 5: T·∫†M KH√ìA RAG ---
# Ch√∫ng ta s·∫Ω kh√¥ng g·ªçi h√†m load_and_process_pdfs() n·ªØa

if "knowledge_data" not in st.session_state:
    print("--- B∆Ø·ªöC 5: RAG ƒêANG B·ªä T·∫ÆT (T·∫†M TH·ªúI) ---") # DEBUG
    # G√°n d·ªØ li·ªáu r·ªóng ƒë·ªÉ app kh√¥ng b·ªã crash ·ªü B∆Ø·ªöC 8
    st.session_state.knowledge_data = ([], None, None) 
    
    # D√≤ng code c≈© (b·ªã t·∫°m kh√≥a):
    # try:
    #     print("--- B∆Ø·ªöC 5: B·∫ÆT ƒê·∫¶U T·∫¢I RAG ---") # DEBUG
    #     with st.spinner("üë©‚Äçüè´ Em ƒëang ƒë·ªçc 'S·ªï tay Tin h·ªçc' (PDF)..."):
    #         knowledge_result = load_and_process_pdfs()
    #         if knowledge_result is None:
    #             st.error("L·ªói: Kh√¥ng th·ªÉ t·∫£i ho·∫∑c x·ª≠ l√Ω c√°c file PDF. RAG s·∫Ω b·ªã t·∫Øt.")
    #             st.session_state.knowledge_data = ([], None, None) 
    #         else:
    #             st.session_state.knowledge_data = knowledge_result
    #             print("--- B∆Ø·ªöC 5: T·∫¢I RAG TH√ÄNH C√îNG ---") # DEBUG
    # except Exception as e:
    #     print(f"--- L·ªñI NGHI√äM TR·ªåNG ·ªû B∆Ø·ªöC 5 ---: {e}") # DEBUG
    #     st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫£i RAG: {e}")
    #     st.session_state.knowledge_data = ([], None, None) # G√°n r·ªóng

# --- B∆Ø·ªöC 6: HI·ªÇN TH·ªä L·ªäCH S·ª¨ CHAT ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    avatar = "‚ú®" if message["role"] == "assistant" else "üë§"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# --- B∆Ø·ªöC 7: M√ÄN H√åNH CH√ÄO M·ª™NG V√Ä G·ª¢I √ù ---
logo_path = "LOGO.jpg" 
col1, col2 = st.columns([1, 5])
with col1:
    try:
        st.image(logo_path, width=80)
    except Exception as e:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file logo t√™n l√† '{logo_path}'.")
with col2:
    st.title("KTC. Chatbot h·ªó tr·ª£ m√¥n Tin H·ªçc")

# --- [S·ª¨A L·ªñI NAMEERROR] ---
# Th√™m l·∫°i h√†m b·ªã thi·∫øu m√† t√¥i ƒë√£ v√¥ t√¨nh x√≥a m·∫•t
def set_prompt_from_suggestion(text):
    st.session_state.prompt_from_button = text
# --- K·∫æT TH√öC S·ª¨A L·ªñI NAMEERROR ---

if not st.session_state.messages:
    st.markdown(f"<div class='welcome-message'>Xin ch√†o! Th·∫ßy/em c·∫ßn h·ªó tr·ª£ g√¨ v·ªÅ m√¥n Tin h·ªçc (Ch∆∞∆°ng tr√¨nh 2018)?</div>", unsafe_allow_html=True)
    st.markdown("<br>", unsafe_allow_html=True)
    
    col1_btn, col2_btn = st.columns(2)
    with col1_btn:
        st.button(
            "Gi·∫£i th√≠ch v·ªÅ 'bi·∫øn' trong l·∫≠p tr√¨nh?",
            on_click=set_prompt_from_suggestion, # H√†m n√†y gi·ªù ƒë√£ t·ªìn t·∫°i
            args=("Gi·∫£i th√≠ch v·ªÅ 'bi·∫øn' trong l·∫≠p tr√¨nh?",), 
            use_container_width=True
        )
        st.button(
            "Tr√¨nh b√†y v·ªÅ an to√†n th√¥ng tin?",
            on_click=set_prompt_from_suggestion, 
            args=("Tr√¨nh b√†y v·ªÅ an to√†n th√¥ng tin?",), 
            use_container_width=True
        )
    with col2_btn:
        st.button(
            "S·ª± kh√°c nhau gi·ªØa RAM v√† ROM?",
            on_click=set_prompt_from_suggestion, 
            args=("S·ª± kh√°c nhau gi·ªØa RAM v√† ROM?",), 
            use_container_width=True
        )
        st.button(
            "C√°c b∆∞·ªõc ch√®n ·∫£nh v√†o word",
            on_click=set_prompt_from_suggestion, 
            args=("C√°c b∆∞·ªõc ch√®n ·∫£nh v√†o word?",), 
            use_container_width=True
        )


# --- B∆Ø·ªöC 8: X·ª¨ L√ù INPUT (ƒê√É S·ª¨A L·ªñI TREO) --- 

prompt_from_input = st.chat_input("M·ªùi th·∫ßy ho·∫∑c c√°c em ƒë·∫∑t c√¢u h·ªèi v·ªÅ Tin h·ªçc...")
prompt_from_button = st.session_state.pop("prompt_from_button", None)
prompt = prompt_from_button or prompt_from_input

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="üë§"):
        st.markdown(prompt)

    try:
        with st.chat_message("assistant", avatar="‚ú®"):
            placeholder = st.empty()
            
            with placeholder.status("üë©‚Äçüè´ Chatbook ƒëang suy nghƒ©..."):
                print("--- B∆Ø·ªöC 8: B·∫ÆT ƒê·∫¶U X·ª¨ L√ù PROMPT ---") # DEBUG
                
                # --- PH·∫¶N RAG (ƒêANG B·ªä T·∫ÆT) ---
                # st.session_state.knowledge_data gi·ªù l√† ([], None, None)
                if st.session_state.knowledge_data and st.session_state.knowledge_data[0]:
                    print("ƒêang t√¨m ki·∫øn th·ª©c li√™n quan...") # DEBUG
                    retrieved_context = find_relevant_knowledge(prompt, st.session_state.knowledge_data, num_chunks=3)
                else:
                    retrieved_context = [] # S·∫Ω lu√¥n r·ªóng v√¨ RAG b·ªã t·∫Øt
                
                print("ƒêang chuy·ªÉn ƒë·ªïi l·ªãch s·ª≠ chat...") # DEBUG
                messages_for_api = convert_history_for_gemini(st.session_state.messages)
                
                if retrieved_context:
                    # S·∫Ω kh√¥ng bao gi·ªù ch·∫°y v√†o ƒë√¢y v√¨ RAG ƒë√£ t·∫Øt
                    print(f"ƒê√£ t√¨m th·∫•y {len(retrieved_context)} m·∫©u ki·∫øn th·ª©c RAG.") # DEBUG
                    context_message = (
                        "--- B·∫ÆT ƒê·∫¶U D·ªÆ LI·ªÜU TRA C·ª®U T·ª™ 'S·ªî TAY' (RAG) ---\n"
                        "ƒê√¢y l√† th√¥ng tin b·ªï sung t·ª´ 'S·ªï tay Tin h·ªçc' c·ªßa b·∫°n. "
                        "H√£y s·ª≠ d·ª•ng th√¥ng tin n√†y l√†m NGU·ªíN ∆ØU TI√äN ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n\n"
                    )
                    for i, chunk_text in enumerate(retrieved_context):
                        context_message += f"--- NGU·ªíN {i+1} ---\n{chunk_text}\n\n"
                    context_message += "--- K·∫æT TH√öC D·ªÆ LI·ªÜU TRA C·ª®U ---\n"
                    
                    last_user_message = messages_for_api.pop()
                    new_prompt_content = f"{context_message}\n\nC√¢u h·ªèi: {last_user_message['parts'][0]}"
                    messages_for_api.append({'role': 'user', 'parts': [new_prompt_content]})
                    
                else:
                    print("Kh√¥ng t√¨m th·∫•y ki·∫øn th·ª©c RAG li√™n quan (do RAG ƒë√£ t·∫Øt).") # DEBUG

                # --- [S·ª¨A L·ªñI TREO] ---
                print("ƒêANG G·ªåI API GEMINI...") # DEBUG
                response = gemini_model.generate_content(
                    messages_for_api # G·ª≠i to√†n b·ªô
                )
                print("ƒê√É NH·∫¨N PH·∫¢N H·ªíI T·ª™ GEMINI.") # DEBUG
                
                if not response.parts:
                    if response.candidates and response.candidates[0].finish_reason == "SAFETY":
                        bot_response_text = "Xin l·ªói, c√¢u tr·∫£ l·ªùi c·ªßa t√¥i ƒë√£ b·ªã ch·∫∑n v√¨ l√Ω do an to√†n. Th·∫ßy/em vui l√≤ng h·ªèi kh√°c ƒëi."
                    else:
                        bot_response_text = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi n√†y."
                else:
                    bot_response_text = response.text
                
                # --- [K·∫æT TH√öC S·ª¨A L·ªñI] ---

            placeholder.markdown(bot_response_text)

    except Exception as e:
        with st.chat_message("assistant", avatar="‚ú®"):
            st.error(f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi k·∫øt n·ªëi Gemini: {e}")
            print(f"--- L·ªñI X·∫¢Y RA ·ªû B∆Ø·ªöC 8 ---: {e}") # DEBUG
        bot_response_text = ""

    # Th√™m c√¢u tr·∫£ l·ªùi c·ªßa bot v√†o l·ªãch s·ª≠
    if bot_response_text:
        st.session_state.messages.append({"role": "assistant", "content": bot_response_text})

    # Rerun n·∫øu b·∫•m n√∫t
    if prompt_from_button:
        st.rerun()