import streamlit as st
from groq import Groq
import os
import glob
import time
from pypdf import PdfReader

# --- LIBRARY AI & RAG ---
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG & H·∫∞NG S·ªê ---
st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω Tin h·ªçc 2025",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

CONSTANTS = {
    "MODEL_NAME": 'llama-3.1-8b-instant',
    "PDF_DIR": "./PDF_KNOWLEDGE",
    "VECTOR_STORE_PATH": "./faiss_db_index", # N∆°i l∆∞u b·ªô n√£o vƒ©nh vi·ªÖn
    "LOGO_PATH": "LOGO.jpg",
    "EMBEDDING_MODEL": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", # T·ªët h∆°n cho ti·∫øng Vi·ªát
    "CHUNK_SIZE": 800, # Gi·∫£m chunk size ƒë·ªÉ n·ªôi dung c√¥ ƒë·ªçng h∆°n
    "CHUNK_OVERLAP": 150
}

# --- 2. CLASS X·ª¨ L√ù RAG (OOP STRUCTURE) ---
class KnowledgeBase:
    """Class qu·∫£n l√Ω vi·ªác ƒë·ªçc, x·ª≠ l√Ω v√† truy xu·∫•t d·ªØ li·ªáu ki·∫øn th·ª©c."""
    
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=CONSTANTS["EMBEDDING_MODEL"])

    def load_documents(self):
        """ƒê·ªçc PDF t·ª´ th∆∞ m·ª•c."""
        if not os.path.exists(CONSTANTS["PDF_DIR"]):
            os.makedirs(CONSTANTS["PDF_DIR"])
            return []
        
        pdf_files = glob.glob(os.path.join(CONSTANTS["PDF_DIR"], "*.pdf"))
        documents = []
        
        for pdf_path in pdf_files:
            try:
                reader = PdfReader(pdf_path)
                file_name = os.path.basename(pdf_path)
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text:
                        documents.append(Document(
                            page_content=text, 
                            metadata={"source": file_name, "page": i + 1}
                        ))
            except Exception as e:
                st.warning(f"Kh√¥ng th·ªÉ ƒë·ªçc file {pdf_path}: {e}")
        return documents

    def build_or_load_vector_db(self, force_rebuild=False):
        """
        C∆° ch·∫ø th√¥ng minh:
        1. Ki·ªÉm tra xem ƒë√£ c√≥ Database l∆∞u tr√™n ·ªï c·ª©ng ch∆∞a.
        2. N·∫øu c√≥ -> Load l√™n (m·∫•t 1 gi√¢y).
        3. N·∫øu ch∆∞a ho·∫∑c user √©p bu·ªôc -> X√¢y d·ª±ng l·∫°i (m·∫•t nhi·ªÅu th·ªùi gian).
        """
        if os.path.exists(CONSTANTS["VECTOR_STORE_PATH"]) and not force_rebuild:
            try:
                # Load t·ª´ ·ªï c·ª©ng
                return FAISS.load_local(
                    CONSTANTS["VECTOR_STORE_PATH"], 
                    self.embeddings, 
                    allow_dangerous_deserialization=True
                )
            except Exception:
                pass # N·∫øu l·ªói file th√¨ build l·∫°i

        # N·∫øu ch∆∞a c√≥, b·∫Øt ƒë·∫ßu build
        documents = self.load_documents()
        if not documents:
            return None

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CONSTANTS["CHUNK_SIZE"], 
            chunk_overlap=CONSTANTS["CHUNK_OVERLAP"]
        )
        splits = text_splitter.split_documents(documents)
        
        if not splits: return None

        # T·∫°o Vector Store
        vector_db = FAISS.from_documents(splits, self.embeddings)
        # L∆∞u xu·ªëng ·ªï c·ª©ng ƒë·ªÉ l·∫ßn sau d√πng
        vector_db.save_local(CONSTANTS["VECTOR_STORE_PATH"])
        return vector_db

# --- 3. GIAO DI·ªÜN & LOGIC CH√çNH ---

# CSS Tinh ch·ªânh (Gi·ªØ nguy√™n style ƒë·∫πp c·ªßa th·∫ßy, t·ªëi ∆∞u th√™m font)
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap');
    html, body, [class*="css"] { font-family: 'Roboto', sans-serif; }
    .stApp {background-color: #f8f9fa;}
    
    /* Sidebar Pro */
    [data-testid="stSidebar"] { background-color: #ffffff; border-right: 1px solid #e0e0e0; }
    
    /* Chat Bubble Pro */
    div[data-testid="stChatMessage"] { padding: 1rem; border-radius: 10px; }
    div[data-testid="stChatMessage"]:nth-child(odd) { background-color: #f0f9ff; border: 1px solid #bae6fd; }
    div[data-testid="stChatMessage"]:nth-child(even) { background-color: #ffffff; border: 1px solid #e2e8f0; box-shadow: 0 2px 5px rgba(0,0,0,0.05); }
    
    .gradient-text {
        background: linear-gradient(90deg, #0052cc, #00c6ff);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-weight: 800;
        font-size: 2.2rem;
        text-align: center;
        padding: 10px 0;
    }
    
    .source-box {
        font-size: 0.8rem; color: #555; background: #f1f1f1; 
        padding: 8px; border-radius: 5px; margin-top: 5px; border-left: 3px solid #0284c7;
    }
</style>
""", unsafe_allow_html=True)

# Kh·ªüi t·∫°o k·∫øt n·ªëi Groq
try:
    api_key = st.secrets["GROQ_API_KEY"]
    client = Groq(api_key=api_key)
except Exception:
    st.error("‚ö†Ô∏è L·ªói h·ªá th·ªëng: Ch∆∞a c·∫•u h√¨nh API Key. Vui l√≤ng ki·ªÉm tra secrets.toml")
    st.stop()

# Kh·ªüi t·∫°o Session State
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Xin ch√†o! T√¥i l√† **KTC AI**. H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ Tin h·ªçc trong SGK."}
    ]

if "rag_engine" not in st.session_state:
    st.session_state.rag_engine = KnowledgeBase()

# Load Vector DB (Ch·ªâ load 1 l·∫ßn ƒë·∫ßu, c·ª±c nhanh)
if "vector_db" not in st.session_state:
    with st.spinner('üîÑ ƒêang k√≠ch ho·∫°t h·ªá th·ªëng tri th·ª©c s·ªë...'):
        st.session_state.vector_db = st.session_state.rag_engine.build_or_load_vector_db()

# --- SIDEBAR ---
with st.sidebar:
    if os.path.exists(CONSTANTS["LOGO_PATH"]):
        st.image(CONSTANTS["LOGO_PATH"], use_container_width=True)
    
    st.title("‚öôÔ∏è Control Panel")
    
    # Tr·∫°ng th√°i h·ªá th·ªëng
    status_color = "green" if st.session_state.vector_db else "red"
    status_text = "ƒê√£ n·∫°p ki·∫øn th·ª©c" if st.session_state.vector_db else "Ch∆∞a c√≥ d·ªØ li·ªáu"
    st.markdown(f"**Tr·∫°ng th√°i:** <span style='color:{status_color}'>‚óè {status_text}</span>", unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Admin Controls
    if st.button("üîÑ C·∫≠p nh·∫≠t l·∫°i D·ªØ li·ªáu (Re-build)", help="Nh·∫•n khi b·∫°n m·ªõi b·ªè th√™m file PDF v√†o"):
        with st.spinner("ƒêang ƒë·ªçc l·∫°i to√†n b·ªô t√†i li·ªáu... (S·∫Ω m·∫•t th·ªùi gian)"):
            st.session_state.vector_db = st.session_state.rag_engine.build_or_load_vector_db(force_rebuild=True)
        st.success("ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi!")
        time.sleep(1)
        st.rerun()

    if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ Chat"):
        st.session_state.messages = []
        st.rerun()

    # Author Info (Gi·ªØ nguy√™n format c·ªßa th·∫ßy)
    st.markdown("""
    <div style="background:#f8f9fa; padding:15px; border-radius:8px; border:1px dashed #ccc; margin-top:20px;">
        <div style="font-weight:bold; color:#0052cc; font-size:0.9rem;">üöÄ D·ª∞ √ÅN KHKT 2025-2026</div>
        <div style="font-size:0.85rem; margin-top:5px;">GVHD: <b>Th·∫ßy Nguy·ªÖn Th·∫ø Khanh</b></div>
        <div style="font-size:0.85rem;">H·ªçc sinh: <b>B√πi T√° T√πng - Cao S·ªπ B·∫£o Chung</b></div>
    </div>
    """, unsafe_allow_html=True)

# --- MAIN CHAT INTERFACE ---
col1, col2, col3 = st.columns([1, 10, 1])

with col2:
    st.markdown('<h1 class="gradient-text">TR·ª¢ L√ù ·∫¢O TIN H·ªåC KTC</h1>', unsafe_allow_html=True)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if message["role"] == "user" else "ü§ñ"
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"], unsafe_allow_html=True)

    # X·ª≠ l√Ω khi ng∆∞·ªùi d√πng nh·∫≠p li·ªáu
    if prompt := st.chat_input("B·∫°n mu·ªën t√¨m hi·ªÉu g√¨ v·ªÅ Tin h·ªçc?"):
        # 1. Hi·ªÉn th·ªã c√¢u h·ªèi user
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        # 2. X·ª≠ l√Ω RAG
        with st.chat_message("assistant", avatar="ü§ñ"):
            message_placeholder = st.empty()
            full_response = ""
            
            # T√¨m ki·∫øm context
            context_text = ""
            sources = []
            
            if st.session_state.vector_db:
                # Similarity Search
                results = st.session_state.vector_db.similarity_search(prompt, k=3)
                for doc in results:
                    context_text += f"\n[N·ªôi dung tr√≠ch xu·∫•t]: {doc.page_content}\n[Ngu·ªìn]: {doc.metadata.get('source')} - Trang {doc.metadata.get('page')}"
                    sources.append(f"{doc.metadata.get('source')} (Tr. {doc.metadata.get('page')})")
            
            # Prompt Engineering Cao c·∫•p (Instruction Tuning)
            SYSTEM_PROMPT = f"""
            B·∫°n l√† tr·ª£ l√Ω ·∫£o KTC, chuy√™n gia v·ªÅ m√¥n Tin h·ªçc GDPT 2018.
            NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.
            
            Y√äU C·∫¶U:
            1. Gi·ªçng vƒÉn th√¢n thi·ªán, s∆∞ ph·∫°m, d·ªÖ hi·ªÉu cho h·ªçc sinh.
            2. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin trong ph·∫ßn [TH√îNG TIN T√ÄI LI·ªÜU] ƒë·ªÉ tr·∫£ l·ªùi. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i "SGK hi·ªán ch∆∞a ƒë·ªÅ c·∫≠p v·∫•n ƒë·ªÅ n√†y".
            3. Tr√¨nh b√†y ƒë·∫πp: S·ª≠ d·ª•ng Markdown (in ƒë·∫≠m, g·∫°ch ƒë·∫ßu d√≤ng).
            
            [TH√îNG TIN T√ÄI LI·ªÜU]:
            {context_text}
            """
            
            # Streaming Response
            try:
                stream = client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        # G·ª≠i k√®m v√†i tin nh·∫Øn c≈© ƒë·ªÉ AI hi·ªÉu ng·ªØ c·∫£nh (Context Window)
                        *st.session_state.messages[-4:], 
                    ],
                    model=CONSTANTS["MODEL_NAME"],
                    stream=True,
                    temperature=0.3, # Gi·ªØ nhi·ªát ƒë·ªô th·∫•p ƒë·ªÉ c√¢u tr·∫£ l·ªùi ch√≠nh x√°c theo s√°ch
                    max_tokens=1024
                )

                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        message_placeholder.markdown(full_response + "‚ñå")
                
                # Hi·ªÉn th·ªã ngu·ªìn t√†i li·ªáu (Tr√≠ch d·∫´n khoa h·ªçc)
                if sources:
                    unique_sources = list(set(sources))
                    source_html = "<div class='source-box'>üìö <b>Ngu·ªìn tham kh·∫£o x√°c th·ª±c:</b><br>" + "<br>".join([f"‚Ä¢ {s}" for s in unique_sources]) + "</div>"
                    final_content = full_response + "\n\n" + source_html
                    message_placeholder.markdown(final_content, unsafe_allow_html=True)
                    st.session_state.messages.append({"role": "assistant", "content": final_content})
                else:
                    message_placeholder.markdown(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})

            except Exception as e:
                st.error(f"ƒê√£ x·∫£y ra l·ªói k·∫øt n·ªëi AI: {str(e)}")