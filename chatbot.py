import streamlit as st
from groq import Groq
import os
import glob
import time
from pypdf import PdfReader
# --- C√ÅC TH∆Ø VI·ªÜN RAG CHU·∫®N (FAISS + EMBEDDINGS) ---
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

# --- C·∫§U H√åNH ---
st.set_page_config(page_title="Chatbot Tin h·ªçc KTC", page_icon="ü§ñ", layout="centered")
MODEL_NAME = 'llama-3.1-8b-instant'
PDF_DIR = "./PDF_KNOWLEDGE" # Th∆∞ m·ª•c ch·ª©a SGK PDF

# --- CSS GIAO DI·ªÜN (Gi·ªØ nguy√™n phong c√°ch c·ªßa th·∫ßy) ---
st.markdown("""
<style>
    [data-testid="stSidebar"] {background-color: #f8f9fa; border-right: 1px solid #e6e6e6;}
    .main .block-container {max-width: 850px; padding-top: 2rem; padding-bottom: 5rem;}
    .stButton>button {border-radius: 20px; height: 3em; background-color: #ffffff; border: 1px solid #d0d0d0;}
    .stButton>button:hover {border-color: #4CAF50; color: #4CAF50;}
    .chat-message {padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex;}
    .chat-message.user {background-color: #e6f7ff;}
    .chat-message.bot {background-color: #f0f2f6;}
</style>
""", unsafe_allow_html=True)

# --- L·∫§Y API KEY ---
try:
    api_key = st.secrets["GROQ_API_KEY"]
except (KeyError, FileNotFoundError):
    st.error("‚ùå L·ªói: Ch∆∞a c·∫•u h√¨nh GROQ_API_KEY trong .streamlit/secrets.toml")
    st.stop()

client = Groq(api_key=api_key)

# --- H·ªÜ TH·ªêNG RAG: FAISS + EMBEDDINGS ---
@st.cache_resource(show_spinner=False)
def initialize_vector_db():
    """
    H√†m n√†y ƒë·ªçc PDF, t·∫°o Embeddings v√† x√¢y d·ª±ng Vector Store (FAISS).
    Ch·∫°y 1 l·∫ßn duy nh·∫•t khi kh·ªüi ƒë·ªông app ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô.
    """
    vector_db = None
    
    # 1. Ki·ªÉm tra th∆∞ m·ª•c PDF
    if not os.path.exists(PDF_DIR):
        os.makedirs(PDF_DIR)
        return None
    
    pdf_files = glob.glob(os.path.join(PDF_DIR, "*.pdf"))
    if not pdf_files:
        return None

    with st.spinner('üîÑ ƒêang kh·ªüi t·∫°o "B·ªô n√£o" ki·∫øn th·ª©c (Vector h√≥a d·ªØ li·ªáu)...'):
        # 2. ƒê·ªçc v√† Chia nh·ªè vƒÉn b·∫£n (Chunking)
        documents = []
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,    # K√≠ch th∆∞·ªõc m·ªói ƒëo·∫°n (kho·∫£ng 2-3 ƒëo·∫°n vƒÉn)
            chunk_overlap=200,  # Ph·∫ßn ch·ªìng l·∫•n ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh
            separators=["\n\n", "\n", ".", " ", ""]
        )

        for pdf_path in pdf_files:
            try:
                reader = PdfReader(pdf_path)
                file_name = os.path.basename(pdf_path)
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text:
                        # L∆∞u th√™m metadata (t√™n s√°ch, s·ªë trang) ƒë·ªÉ tr√≠ch d·∫´n ngu·ªìn
                        chunks = text_splitter.split_text(text)
                        for chunk in chunks:
                            documents.append(Document(
                                page_content=chunk,
                                metadata={"source": file_name, "page": i + 1}
                            ))
            except Exception as e:
                print(f"L·ªói ƒë·ªçc file {pdf_path}: {e}")

        if not documents:
            return None

        # 3. T·∫°o Embeddings (S·ª≠ d·ª•ng Model thu nh·ªè c·ªßa HuggingFace - Ch·∫°y Offline OK)
        # Model n√†y bi·∫øn vƒÉn b·∫£n th√†nh vector 384 chi·ªÅu
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

        # 4. T·∫°o FAISS Index (Vector Database)
        vector_db = FAISS.from_documents(documents, embeddings)
        
        print(f"‚úÖ ƒê√£ kh·ªüi t·∫°o th√†nh c√¥ng Vector DB v·ªõi {len(documents)} chunks ki·∫øn th·ª©c.")
        
    return vector_db

# --- KH·ªûI T·∫†O SESSION STATE ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = initialize_vector_db()

# --- SIDEBAR ---
with st.sidebar:
    st.title("ü§ñ Chatbot KTC")
    st.caption("Tr·ª£ l√Ω h·ªçc t·∫≠p m√¥n Tin h·ªçc")
    st.markdown("---")
    
    # Hi·ªÉn th·ªã tr·∫°ng th√°i h·ªá th·ªëng RAG
    if st.session_state.vector_db:
        st.success("‚úÖ K·∫øt n·ªëi tri th·ª©c SGK: ƒê√£ s·∫µn s√†ng", icon="üìö")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu SGK. Vui l√≤ng ch√©p file PDF v√†o th∆∞ m·ª•c PDF_KNOWLEDGE.", icon="üìÇ")
        
    if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
        st.session_state.messages = []
        st.rerun()
        
    st.markdown("---")
    st.info("**GVHD:** Th·∫ßy Nguy·ªÖn Th·∫ø Khanh\n\n**H·ªçc sinh:**\n- B√πi T√° T√πng\n- Cao S·ªπ B·∫£o Chung")

# --- PROMPT K·ª∏ S∆Ø (SYSTEM INSTRUCTION) ---
SYSTEM_PROMPT = """
B·∫°n l√† "Chatbot KTC", tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ h·ªçc t·∫≠p m√¥n Tin h·ªçc theo Ch∆∞∆°ng tr√¨nh GDPT 2018 (B·ªô s√°ch K·∫øt n·ªëi tri th·ª©c, C√°nh Di·ªÅu, Ch√¢n tr·ªùi s√°ng t·∫°o).
Phong c√°ch tr·∫£ l·ªùi:
1. S∆∞ ph·∫°m, d·ªÖ hi·ªÉu, th√¢n thi·ªán nh∆∞ m·ªôt gi√°o vi√™n gi·ªèi.
2. Lu√¥n ∆∞u ti√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn "B·ªêI C·∫¢NH TRA C·ª®U".
3. N·∫øu th√¥ng tin c√≥ trong B·ªêI C·∫¢NH, h√£y tr√≠ch d·∫´n ngu·ªìn (V√≠ d·ª•: Theo SGK Tin h·ªçc 10...).
4. N·∫øu B·ªêI C·∫¢NH kh√¥ng ch·ª©a th√¥ng tin tr·∫£ l·ªùi, h√£y d√πng ki·∫øn th·ª©c c·ªßa b·∫°n nh∆∞ng ph·∫£i n√≥i r√µ: "Th√¥ng tin n√†y kh√¥ng c√≥ trong t√†i li·ªáu tham kh·∫£o, nh∆∞ng theo ki·∫øn th·ª©c c·ªßa t√¥i th√¨...".
"""

# --- X·ª¨ L√ù CHAT ---
# 1. Hi·ªÉn th·ªã l·ªãch s·ª≠
for message in st.session_state.messages:
    role_icon = "üë§" if message["role"] == "user" else "ü§ñ"
    with st.chat_message(message["role"], avatar=role_icon):
        st.markdown(message["content"])

# 2. Nh·∫≠n c√¢u h·ªèi
prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi v·ªÅ m√¥n Tin h·ªçc (VD: M·∫°ng m√°y t√≠nh l√† g√¨?)...")

if prompt:
    # Th√™m c√¢u h·ªèi v√†o l·ªãch s·ª≠
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="üë§"):
        st.markdown(prompt)

    # --- LOGIC RAG (RETRIEVAL) ---
    context_text = ""
    sources_list = []
    
    if st.session_state.vector_db:
        # T√¨m ki·∫øm 3 ƒëo·∫°n vƒÉn b·∫£n t∆∞∆°ng ƒë·ªìng nh·∫•t (Semantic Search)
        # k=3 nghƒ©a l√† l·∫•y 3 ƒëo·∫°n li√™n quan nh·∫•t
        results = st.session_state.vector_db.similarity_search(prompt, k=3)
        
        if results:
            for doc in results:
                context_text += f"\n---\nN·ªôi dung: {doc.page_content}\nNgu·ªìn: {doc.metadata['source']} (Trang {doc.metadata['page']})"
                sources_list.append(f"{doc.metadata['source']} (Trang {doc.metadata['page']})")

    # --- T·∫†O PROMPT CU·ªêI C√ôNG G·ª¨I CHO LLM ---
    final_prompt = f"""
    {SYSTEM_PROMPT}
    
    --- B·∫ÆT ƒê·∫¶U B·ªêI C·∫¢NH TRA C·ª®U (TH√îNG TIN T·ª™ SGK) ---
    {context_text if context_text else "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."}
    --- K·∫æT TH√öC B·ªêI C·∫¢NH ---
    
    C√¢u h·ªèi c·ªßa h·ªçc sinh: {prompt}
    """

    # --- G·ªåI API GROQ ---
    with st.chat_message("assistant", avatar="ü§ñ"):
        placeholder = st.empty()
        full_response = ""
        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {"role": "system", "content": final_prompt},
                    {"role": "user", "content": prompt}
                ],
                model=MODEL_NAME,
                stream=True,
                temperature=0.3 # Gi·∫£m ƒë·ªô s√°ng t·∫°o ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
            )

            for chunk in chat_completion:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    placeholder.markdown(full_response + "‚ñå")
            
            # Th√™m ph·∫ßn tr√≠ch d·∫´n ngu·ªìn v√†o cu·ªëi c√¢u tr·∫£ l·ªùi (ƒêi·ªÉm c·ªông cho KHKT)
            if sources_list:
                citation_text = "\n\n---\nBadges: *" + ", ".join(list(set(sources_list))) + "*"
                full_response += citation_text
                
            placeholder.markdown(full_response)
            
            # L∆∞u v√†o l·ªãch s·ª≠
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            st.error(f"ƒê√£ x·∫£y ra l·ªói k·∫øt n·ªëi: {e}")