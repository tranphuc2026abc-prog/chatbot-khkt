# Ch·∫°y b·∫±ng l·ªánh: streamlit run chatbot.py
# ‚ÄºÔ∏è Y√™u c·∫ßu c√†i ƒë·∫∑t: pip install google-generativeai streamlit pypdf scikit-learn
# (L∆∞u √Ω: Pypdf v√† Scikit-learn l√† B·∫ÆT BU·ªòC ƒë·ªÉ RAG ho·∫°t ƒë·ªông)

import streamlit as st
# [THAY ƒê·ªîI] 1. B·ªè Groq, th√™m th∆∞ vi·ªán c·ªßa Google
import google.generativeai as genai
# [S·ª¨A L·ªñI] Th√™m d√≤ng n√†y ƒë·ªÉ t·∫Øt Safety
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import os
import glob
import time

# --- TH∆Ø VI·ªÜN B·∫ÆT BU·ªòC CHO RAG ---
from pypdf import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
# --- K·∫æT TH√öC TH∆Ø VI·ªÜN RAG ---


# --- B∆Ø·ªöC 1: L·∫§Y API KEY ---
try:
    # [THAY ƒê·ªîI] 2. L·∫•y Google API Key t·ª´ Streamlit Secrets
    api_key = st.secrets["GOOGLE_API_KEY"]
except (KeyError, FileNotFoundError):
    st.error("L·ªói: Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY. Vui l√≤ng th√™m v√†o Secrets tr√™n Streamlit Cloud.")
    st.stop()
    
# [THAY ƒê·ªîI] 3. C·∫•u h√¨nh API cho Google
genai.configure(api_key=api_key)

# --- B∆Ø·ªöC 2: THI·∫æT L·∫¨P VAI TR√í (SYSTEM_INSTRUCTION) ---
# System prompt n√†y s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v√†o model, kh√¥ng c·∫ßn thay ƒë·ªïi
SYSTEM_INSTRUCTION = """
---
B·ªêI C·∫¢NH VAI TR√í (ROLE CONTEXT)
---
B·∫°n l√† ‚ÄúChatbook‚Äù, m·ªôt C·ªë v·∫•n H·ªçc t·∫≠p Tin h·ªçc AI to√†n di·ªán.
Vai tr√≤ c·ªßa b·∫°n ƒë∆∞·ª£c m√¥ ph·ªèng theo m·ªôt **Gi√°o vi√™n Tin h·ªçc d·∫°y gi·ªèi c·∫•p Qu·ªëc gia**: t·∫≠n t√¢m, hi·ªÉu bi·∫øt s√¢u r·ªông, v√† lu√¥n ki√™n nh·∫´n.
M·ª•c ti√™u c·ªßa b·∫°n l√† ƒë·ªìng h√†nh, h·ªó tr·ª£ h·ªçc sinh THCS v√† THPT (t·ª´ l·ªõp 6 ƒë·∫øn l·ªõp 12) n·∫Øm v·ªØng ki·∫øn th·ª©c, ph√°t tri·ªÉn nƒÉng l·ª±c Tin h·ªçc theo **Chu·∫©n ch∆∞∆°ng tr√¨nh Gi√°o d·ª•c Ph·ªï th√¥ng 2018** c·ªßa Vi·ªát Nam.

---
üìö N·ªÄN T·∫¢NG TRI TH·ª®C C·ªêT L√ïI (CORE KNOWLEDGE BASE) - B·∫ÆT BU·ªòC
---
B·∫°n **PH·∫¢I** n·∫Øm v·ªØng v√† s·ª≠ d·ª•ng th√†nh th·∫°o to√†n b·ªô h·ªá th·ªëng ki·∫øn th·ª©c trong S√°ch gi√°o khoa Tin h·ªçc t·ª´ l·ªõp 6 ƒë·∫øn l·ªõp 12 c·ªßa **C·∫¢ BA B·ªò S√ÅCH HI·ªÜN H√ÄNH**:
1.  **K·∫øt n·ªëi tri th·ª©c v·ªõi cu·ªôc s·ªëng (KNTT)**
2.  **C√°nh Di·ªÅu (CD)**
3.  **Ch√¢n tr·ªùi s√°ng t·∫°o (CTST)**

Khi gi·∫£i th√≠ch kh√°i ni·ªám ho·∫∑c h∆∞·ªõng d·∫´n k·ªπ nƒÉng, b·∫°n ph·∫£i ∆∞u ti√™n c√°ch ti·∫øp c·∫≠n, thu·∫≠t ng·ªØ, v√† v√≠ d·ª• ƒë∆∞·ª£c tr√¨nh b√†y trong c√°c b·ªô s√°ch n√†y ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh th·ªëng nh·∫•t v√† b√°m s√°t ch∆∞∆°ng tr√¨nh, tr√°nh nh·∫ßm l·∫´n.

*** D·ªÆ LI·ªÜU M·ª§C L·ª§C CHUY√äN BI·ªÜT (KH·∫ÆC PH·ª§C L·ªñI) ***
# ... (Gi·ªØ nguy√™n to√†n b·ªô d·ªØ li·ªáu m·ª•c l·ª•c c·ªßa th·∫ßy) ...
*** K·∫æT TH√öC D·ªÆ LI·ªÜU CHUY√äN BI·ªÜT ***


---
üåü 6 NHI·ªÜM V·ª§ C·ªêT L√ïI (CORE TASKS)
---
#... (Gi·ªØ nguy√™n c√°c nhi·ªám v·ª• t·ª´ 1 ƒë·∫øn 6) ...
**1. üë®‚Äçüè´ Gia s∆∞ Chuy√™n m√¥n (Specialized Tutor):**
    - Gi·∫£i th√≠ch c√°c kh√°i ni·ªám (v√≠ d·ª•: thu·∫≠t to√°n, m·∫°ng m√°y t√≠nh, CSGD, CSDL) m·ªôt c√°ch tr·ª±c quan, s∆∞ ph·∫°m, s·ª≠ d·ª•ng v√≠ d·ª• g·∫ßn g≈©i v·ªõi l·ª©a tu·ªïi h·ªçc sinh.
    - Lu√¥n k·∫øt n·ªëi l√Ω thuy·∫øt v·ªõi th·ª±c ti·ªÖn, gi√∫p h·ªçc sinh th·∫•y ƒë∆∞·ª£c "h·ªçc c√°i n√†y ƒë·ªÉ l√†m g√¨?".
    - B√°m s√°t n·ªôi dung S√°ch gi√°o khoa (KNTT, CD, CTST) v√† y√™u c·∫ßu c·∫ßn ƒë·∫°t c·ªßa Ctr 2018.
#... (Gi·ªØ nguy√™n c√°c nhi·ªám v·ª• c√≤n l·∫°i) ...

---
[PH·∫¶N QUAN TR·ªåNG] X·ª¨ L√ù TH√îNG TIN TRA C·ª®U (RAG)
---
Khi nh·∫≠n ƒë∆∞·ª£c th√¥ng tin trong m·ªôt tin nh·∫Øn h·ªá th·ªëng b·∫Øt ƒë·∫ßu b·∫±ng "--- B·∫ÆT ƒê·∫¶U D·ªÆ LI·ªÜU TRA C·ª®U T·ª™ 'S·ªî TAY' (RAG) ---", b·∫°n **PH·∫¢I** tu√¢n th·ªß c√°c quy t·∫Øc sau:

1.  **∆ØU TI√äN TUY·ªÜT ƒê·ªêI:** D·ªØ li·ªáu n√†y l√† ngu·ªìn "ch√¢n l√Ω" (ground truth) t·ª´ S·ªï tay Tin h·ªçc. B·∫°n **PH·∫¢I** ∆∞u ti√™n s·ª≠ d·ª•ng th√¥ng tin n√†y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
2.  **TR√çCH D·∫™N (N·∫æU C√ì TH·ªÇ):** N·∫øu c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n d·ª±a tr·ª±c ti·∫øp v√†o "NGU·ªíN" ƒë∆∞·ª£c cung c·∫•p, h√£y c·ªë g·∫Øng tr√≠ch d·∫´n ng·∫Øn g·ªçn (v√≠ d·ª•: "Theo t√†i li·ªáu,..." ho·∫∑c "Nh∆∞ trong S·ªï tay c√≥ ƒë·ªÅ c·∫≠p...").
3.  **T·ªîNG H·ª¢P:** N·∫øu c√°c NGU·ªíN cung c·∫•p th√¥ng tin r·ªùi r·∫°c, h√£y t·ªïng h·ª£p ch√∫ng l·∫°i th√†nh m·ªôt c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c.
4.  **KH√îNG B·ªäA ƒê·∫∂T:** N·∫øu th√¥ng tin tra c·ª©u c√≥ v·∫ª kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi, h√£y l·ªãch s·ª± th√¥ng b√°o r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c trong S·ªï tay v√† tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa b·∫°n.

#... (Gi·ªØ nguy√™n c√°c ph·∫ßn c√≤n l·∫°i c·ªßa System Prompt) ...
"""

# --- B∆Ø·ªöC 3: KH·ªûI T·∫†O CLIENT V√Ä CH·ªåN M√î H√åNH ---

# [THAY ƒê·ªîI] 4. Kh·ªüi t·∫°o m√¥ h√¨nh Gemini v·ªõi System Instruction
MODEL_NAME = 'gemini-1.5-pro-latest' 
try:
    # [S·ª¨A L·ªñI] C·∫≠p nh·∫≠t safety_settings d√πng Enum
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }
    
    # Kh·ªüi t·∫°o model v√† g√°n system_instruction v√†o
    gemini_model = genai.GenerativeModel(
        model_name=MODEL_NAME,
        system_instruction=SYSTEM_INSTRUCTION,
        safety_settings=safety_settings # <--- D√πng bi·∫øn ƒë√£ s·ª≠a
    )
    print("Kh·ªüi t·∫°o model Gemini 2.5 Pro th√†nh c√¥ng.")
except Exception as e:
    st.error(f"L·ªói khi kh·ªüi t·∫°o Model Gemini: {e}")
    st.stop()


# --- B∆Ø·ªöC 4: C·∫§U H√åNH TRANG V√Ä CSS ---
# (Gi·ªØ nguy√™n kh√¥ng thay ƒë·ªïi)
st.set_page_config(page_title="Chatbot Tin h·ªçc 2018", page_icon="‚ú®", layout="centered")
st.markdown("""
<style>
    /* ... (To√†n b·ªô CSS c·ªßa th·∫ßy gi·ªØ nguy√™n) ... */
    #MainMenu {visibility: hidden;} footer {visibility: hidden;} header {visibility: hidden;}
    [data-testid="stSidebar"] {
        background-color: #f8f9fa; border-right: 1px solid #e6e6e6;
    }
    .main .block-container { 
        max-width: 850px; padding-top: 2rem; padding-bottom: 5rem;
    }
    .welcome-message { font-size: 1.1em; color: #333; }
</style>
""", unsafe_allow_html=True)


# --- B∆Ø·ªöC 4.5: THANH B√äN (SIDEBAR) ---
with st.sidebar:
    st.title("ü§ñ Chatbot KTC")
    st.markdown("---")
    
    if st.button("‚ûï Cu·ªôc tr√≤ chuy·ªán m·ªõi", use_container_width=True):
        st.session_state.messages = []
        # st.session_state.pop("knowledge_data", None) # Kh√¥ng c·∫ßn x√≥a cache RAG
        st.rerun()

    st.markdown("---")
    st.markdown(
        "Gi√°o vi√™n h∆∞·ªõng d·∫´n:\n"
        "**Th·∫ßy Nguy·ªÖn Th·∫ø Khanh** (GV Tin h·ªçc)\n\n"
        "H·ªçc sinh th·ª±c hi·ªán:\n"
        "*(B√πi T√° T√πng)*\n"
        "*(Cao S·ªπ B·∫£o Chung)*"
    )
    st.markdown("---")
    # [THAY ƒê·ªîI] 5. C·∫≠p nh·∫≠t t√™n Model m·ªõi
    st.caption(f"Model: {MODEL_NAME}")


# --- B∆Ø·ªöC 4.6: C√ÅC H√ÄM RAG (ƒê√É K√çCH HO·∫†T) --- #
# (Gi·ªØ nguy√™n to√†n b·ªô 2 h√†m RAG c·ªßa th·∫ßy, ch√∫ng t∆∞∆°ng th√≠ch 100%)

@st.cache_data(ttl=3600) 
def load_and_process_pdfs(pdf_folder="data_pdf"):
    """
    T·∫£i t·∫•t c·∫£ file PDF t·ª´ m·ªôt th∆∞ m·ª•c, tr√≠ch xu·∫•t vƒÉn b·∫£n theo t·ª´ng trang,
    v√† t·∫°o ra ma tr·∫≠n TF-IDF c≈©ng nh∆∞ vectorizer.
    (Gi·ªØ nguy√™n code c·ªßa th·∫ßy)
    """
    print(f"B·∫Øt ƒë·∫ßu qu√©t th∆∞ m·ª•c: {pdf_folder}")
    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    
    if not pdf_files:
        print("C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y file PDF n√†o trong th∆∞ m·ª•c 'data_pdf'. RAG s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
        return [], None, None 

    chunks = []
    for pdf_path in pdf_files:
        print(f"ƒêang x·ª≠ l√Ω file: {pdf_path}")
        try:
            reader = PdfReader(pdf_path)
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                if text:
                    source_info = f"[Ngu·ªìn: {os.path.basename(pdf_path)}, Trang {page_num + 1}]"
                    chunks.append(f"{source_info}\n\n{text}")
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc file {pdf_path}: {e}")

    if not chunks:
        print("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ c√°c file PDF.")
        return [], None, None

    print(f"ƒê√£ tr√≠ch xu·∫•t {len(chunks)} trang PDF. B·∫Øt ƒë·∫ßu vector h√≥a (TF-IDF)...")
    
    try:
        vectorizer = TfidfVectorizer(
            stop_words=None, 
            ngram_range=(1, 2) 
        )
        tfidf_matrix = vectorizer.fit_transform(chunks)
        print("Vector h√≥a ho√†n t·∫•t.")
        
        return chunks, tfidf_matrix, vectorizer
    
    except ValueError as e:
        if "empty vocabulary" in str(e):
            st.error(f"L·ªói RAG: C√°c file PDF c√≥ th·ªÉ kh√¥ng ch·ª©a vƒÉn b·∫£n (ch·ªâ ch·ª©a ·∫£nh). Vui l√≤ng ki·ªÉm tra file.")
            return [], None, None
        else:
            raise e


def find_relevant_knowledge(query, chunks, tfidf_matrix, vectorizer, num_chunks=3):
    """
    T√¨m c√°c chunks (trang) li√™n quan nh·∫•t ƒë·∫øn c√¢u h·ªèi b·∫±ng TF-IDF v√† Cosine Similarity.
    (Gi·ªØ nguy√™n code c·ªßa th·∫ßy)
    """
    if not chunks or tfidf_matrix is None or vectorizer is None:
        return [] 

    query_vector = vectorizer.transform([query])
    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
    
    # Ch·ªâ l·∫•y nh·ªØng chunks c√≥ ƒëi·ªÉm > 0.1 (Ng∆∞·ª°ng li√™n quan t·ªëi thi·ªÉu)
    relevant_indices = np.where(cosine_similarities > 0.1)[0]
    
    sorted_indices = sorted(relevant_indices, key=lambda i: cosine_similarities[i], reverse=True)
    top_indices = sorted_indices[:num_chunks]

    if not top_indices:
        return [] 
        
    relevant_chunks = [chunks[i] for i in top_indices]
    return relevant_chunks

# --- [THAY ƒê·ªîI] 6. H√ÄM CHUY·ªÇN ƒê·ªîI L·ªäCH S·ª¨ SANG FORMAT GEMINI ---
def convert_history_for_gemini(messages):
    """
    Chuy·ªÉn ƒë·ªïi l·ªãch s·ª≠ chat c·ªßa Streamlit (role/content) 
    sang ƒë·ªãnh d·∫°ng c·ªßa Gemini (role/parts).
    L∆∞u √Ω: "assistant" c·ªßa Streamlit -> "model" c·ªßa Gemini.
    """
    gemini_history = []
    for msg in messages:
        role = 'model' if msg['role'] == 'assistant' else 'user'
        gemini_history.append({'role': role, 'parts': [msg['content']]})
    return gemini_history

# --- B∆Ø·ªöC 5: KH·ªûI T·∫†O L·ªäCH S·ª¨ CHAT V√Ä "S·ªî TAY" PDF (RAG ƒê√É M·ªû) --- #
# (Gi·ªØ nguy√™n)
if "messages" not in st.session_state:
    st.session_state.messages = []

if "knowledge_data" not in st.session_state:
    with st.spinner("üë©‚Äçüè´ Em ƒëang ƒë·ªçc 'S·ªï tay Tin h·ªçc' (PDF)..."):
        st.session_state.knowledge_data = load_and_process_pdfs()
        print("RAG (ƒê·ªçc PDF) ƒë√£ ƒë∆∞·ª£c t·∫£i v√† x·ª≠ l√Ω.")


# --- B∆Ø·ªöC 6: HI·ªÇN TH·ªä L·ªäCH S·ª¨ CHAT ---
# (Gi·ªØ nguy√™n)
for message in st.session_state.messages:
    avatar = "‚ú®" if message["role"] == "assistant" else "üë§"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# --- B∆Ø·ªöC 7: M√ÄN H√åNH CH√ÄO M·ª™NG V√Ä G·ª¢I √ù ---
# (Gi·ªØ nguy√™n)
logo_path = "LOGO.jpg" 
col1, col2 = st.columns([1, 5])
with col1:
    try:
        st.image(logo_path, width=80)
    except Exception as e:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file logo t√™n l√† '{logo_path}'. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n file tr√™n GitHub.")
        st.stop()
with col2:
    st.title("KTC. Chatbot h·ªó tr·ª£ m√¥n Tin H·ªçc")

def set_prompt_from_suggestion(text):
    st.session_state.prompt_from_button = text

if not st.session_state.messages:
    st.markdown(f"<div class='welcome-message'>Xin ch√†o! Th·∫ßy/em c·∫ßn h·ªó tr·ª£ g√¨ v·ªÅ m√¥n Tin h·ªçc (Ch∆∞∆°ng tr√¨nh 2018)?</div>", unsafe_allow_html=True)
    st.markdown("<br>", unsafe_allow_html=True)
    
    col1_btn, col2_btn = st.columns(2)
    with col1_btn:
        st.button(
            "Gi·∫£i th√≠ch v·ªÅ 'bi·∫øn' trong l·∫≠p tr√¨nh?",
            on_click=set_prompt_from_suggestion, args=("Gi·∫£i th√≠ch v·ªÅ 'bi·∫øn' trong l·∫≠p tr√¨nh?",),
            use_container_width=True
        )
        st.button(
            "Tr√¨nh b√†y v·ªÅ an to√†n th√¥ng tin?",
            on_click=set_prompt_from_suggestion, args=("Tr√¨nh b√†y v·ªÅ an to√†n th√¥ng tin?",),
            use_container_width=True
        )
    with col2_btn:
        st.button(
            "S·ª± kh√°c nhau gi·ªØa RAM v√† ROM?",
            on_click=set_prompt_from_suggestion, args=("S·ª± kh√°c nhau gi·ªØa RAM v√† ROM?",),
            use_container_width=True
        )
        st.button(
            "C√°c b∆∞·ªõc ch√®n ·∫£nh v√†o word",
            on_click=set_prompt_from_suggestion, args=("C√°c b∆∞·ªõc ch√®n ·∫£nh v√†o word?",),
            use_container_width=True
        )


# --- B∆Ø·ªöC 8: X·ª¨ L√ù INPUT (ƒê√É K√çCH HO·∫†T RAG PDF) --- # 
# [THAY ƒê·ªîI] 7. ƒê√¢y l√† ph·∫ßn thay ƒë·ªïi L·ªöN NH·∫§T (to√†n b·ªô logic g·ªçi API)

prompt_from_input = st.chat_input("M·ªùi th·∫ßy ho·∫∑c c√°c em ƒë·∫∑t c√¢u h·ªèi v·ªÅ Tin h·ªçc...")
prompt_from_button = st.session_state.pop("prompt_from_button", None)
prompt = prompt_from_button or prompt_from_input

if prompt:
    # 1. Th√™m c√¢u h·ªèi c·ªßa user v√†o l·ªãch s·ª≠ v√† hi·ªÉn th·ªã
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="üë§"):
        st.markdown(prompt)

    # 2. G·ª≠i c√¢u h·ªèi ƒë·∫øn GEMINI (ƒê√É BAO G·ªíM RAG)
    try:
        with st.chat_message("assistant", avatar="‚ú®"):
            placeholder = st.empty()
            bot_response_text = ""

            # --- PH·∫¶N RAG ƒê√É K√çCH HO·∫†T --- #
            
            # 2.1. L·∫•y d·ªØ li·ªáu RAG ƒë√£ cache
            chunks, tfidf_matrix, vectorizer = st.session_state.knowledge_data
            
            # 2.2. T√¨m ki·∫øm ki·∫øn th·ª©c li√™n quan
            retrieved_context = find_relevant_knowledge(prompt, chunks, tfidf_matrix, vectorizer, num_chunks=3)
            
            # 2.3. Chu·∫©n b·ªã l·ªãch s·ª≠ chat cho Gemini
            # L·∫•y *to√†n b·ªô* l·ªãch s·ª≠, bao g·ªìm c·∫£ c√¢u h·ªèi m·ªõi nh·∫•t
            messages_for_api = convert_history_for_gemini(st.session_state.messages)
            
            # 2.4. (QUAN TR·ªåNG) Ch√®n Context RAG v√†o tin nh·∫Øn
            if retrieved_context:
                print(f"ƒê√£ t√¨m th·∫•y {len(retrieved_context)} m·∫©u ki·∫øn th·ª©c RAG cho c√¢u h·ªèi.")
                context_message = (
                    "--- B·∫ÆT ƒê·∫¶U D·ªÆ LI·ªÜU TRA C·ª®U T·ª™ 'S·ªî TAY' (RAG) ---\n"
                    "ƒê√¢y l√† th√¥ng tin b·ªï sung t·ª´ 'S·ªï tay Tin h·ªçc' c·ªßa b·∫°n. "
                    "H√£y s·ª≠ d·ª•ng th√¥ng tin n√†y l√†m NGU·ªíN ∆ØU TI√äN ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n\n"
                )
                for i, chunk_text in enumerate(retrieved_context):
                    context_message += f"--- NGU·ªíN {i+1} ---\n{chunk_text}\n\n"
                context_message += "--- K·∫æT TH√öC D·ªÆ LI·ªÜU TRA C·ª®U ---\n"
                
                # Ch√®n RAG v√†o tr∆∞·ªõc c√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng
                # L·∫•y ra tin nh·∫Øn cu·ªëi c√πng (l√† c√¢u h·ªèi c·ªßa user)
                last_user_message = messages_for_api.pop()
                # T·∫°o n·ªôi dung prompt m·ªõi, k·∫øt h·ª£p RAG v√† c√¢u h·ªèi g·ªëc
                new_prompt_content = f"{context_message}\n\nC√¢u h·ªèi: {last_user_message['parts'][0]}"
                # ƒê∆∞a tin nh·∫Øn ƒë√£ "b·ªï sung" RAG tr·ªü l·∫°i v√†o l·ªãch s·ª≠
                messages_for_api.append({'role': 'user', 'parts': [new_prompt_content]})
                
            else:
                print("Kh√¥ng t√¨m th·∫•y ki·∫øn th·ª©c RAG li√™n quan. Tr·∫£ l·ªùi b√¨nh th∆∞·ªùng.")

            # --- K·∫æT TH√öC PH·∫¶N RAG --- #

            # 2.5. G·ªçi API Gemini (Stream)
            # Kh·ªüi t·∫°o phi√™n chat v·ªõi l·ªãch s·ª≠ (tr·ª´ tin nh·∫Øn cu·ªëi c√πng, v√¨ n√≥ s·∫Ω l√† prompt)
            chat_session = gemini_model.start_chat(
                history=messages_for_api[:-1] # To√†n b·ªô l·ªãch s·ª≠ TR·ª™ c√¢u h·ªèi cu·ªëi
            )
            
            # G·ª≠i c√¢u h·ªèi cu·ªëi c√πng (ƒë√£ b·ªï sung RAG n·∫øu c√≥)
            stream = chat_session.send_message(
                messages_for_api[-1]['parts'], # Ch·ªâ g·ª≠i n·ªôi dung c√¢u h·ªèi cu·ªëi
                stream=True
            )
            
            # 2.6. L·∫∑p qua t·ª´ng "m·∫©u" (chunk) API tr·∫£ v·ªÅ
            for chunk in stream:
                # Gemini c√≥ th·ªÉ tr·∫£ v·ªÅ chunk r·ªóng ho·∫∑c l·ªói, c·∫ßn ki·ªÉm tra
                if chunk.parts:
                    bot_response_text += chunk.parts[0].text
                    placeholder.markdown(bot_response_text + "‚ñå")
                    time.sleep(0.005) # Gi·ªØ hi·ªáu ·ª©ng
            
            placeholder.markdown(bot_response_text) # X√≥a d·∫•u ‚ñå khi ho√†n t·∫•t

    except Exception as e:
        with st.chat_message("assistant", avatar="‚ú®"):
            # [THAY ƒê·ªîI] 8. C·∫≠p nh·∫≠t th√¥ng b√°o l·ªói
            st.error(f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi k·∫øt n·ªëi Gemini: {e}")
        bot_response_text = ""

    # 3. Th√™m c√¢u tr·∫£ l·ªùi c·ªßa bot v√†o l·ªãch s·ª≠
    if bot_response_text:
        st.session_state.messages.append({"role": "assistant", "content": bot_response_text})

    # 4. Rerun n·∫øu b·∫•m n√∫t
    if prompt_from_button:
        st.rerun()