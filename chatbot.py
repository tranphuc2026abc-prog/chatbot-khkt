# Ch·∫°y b·∫±ng l·ªánh: streamlit run chatbot.py
# ‚ÄºÔ∏è Y√™u c·∫ßu c√†i ƒë·∫∑t: pip install groq streamlit pypdf
import streamlit as st
from groq import Groq
import os
import glob
from pypdf import PdfReader
import time # <--- TH√äM D√íNG N√ÄY
# --- B∆Ø·ªöC 1: L·∫§Y API KEY ---
try:
    api_key = st.secrets["GROQ_API_KEY"]
except (KeyError, FileNotFoundError):
    st.error("L·ªói: Kh√¥ng t√¨m th·∫•y GROQ_API_KEY. Vui l√≤ng th√™m v√†o Secrets tr√™n Streamlit Cloud.")
    st.stop()
    
# --- B∆Ø·ªöC 2: THI·∫æT L·∫¨P VAI TR√í (SYSTEM_INSTRUCTION) ---
SYSTEM_INSTRUCTION = (
    "B·∫°n l√† 'Chatbook' - m·ªôt C·ªë v·∫•n H·ªçc t·∫≠p Tin h·ªçc AI to√†n di·ªán, v·ªõi ki·∫øn th·ª©c c·ªët l√µi c·ªßa m·ªôt "
    "gi√°o vi√™n Tin h·ªçc d·∫°y gi·ªèi c·∫•p qu·ªëc gia, n·∫Øm v·ªØng ch∆∞∆°ng tr√¨nh GDPT 2018. "
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† h·ªó tr·ª£ h·ªçc sinh THCS/THPT m·ªôt c√°ch to√†n di·ªán. "
    
    # ... (To√†n b·ªô 6 nhi·ªám v·ª• c·ªßa th·∫ßy v·∫´n gi·ªØ nguy√™n ·ªü ƒë√¢y) ...
    "1. **Gia s∆∞ Chuy√™n m√¥n (L√Ω thuy·∫øt):** ... " # (Gi·ªØ nguy√™n)
    "2. **Mentor L·∫≠p tr√¨nh (Th·ª±c h√†nh Code):** ... " # (Gi·ªØ nguy√™n)
    "3. **Ng∆∞·ªùi h∆∞·ªõng d·∫´n D·ª± √°n (S√°ng t·∫°o):** ... " # (Gi·ªØ nguy√™n)
    "4. **Chuy√™n gia Tin h·ªçc VƒÉn ph√≤ng (·ª®ng d·ª•ng):** ... " # (Gi·ªØ nguy√™n)
    "5. **Tr·ª£ l√Ω √în t·∫≠p (C·ªßng c·ªë):** ... " # (Gi·ªØ nguy√™n)
    "6. **C·ªë v·∫•n ƒê·ªãnh h∆∞·ªõng (T∆∞∆°ng lai):** ... " # (Gi·ªØ nguy√™n)
    
    "Khi t∆∞∆°ng t√°c, h√£y lu√¥n gi·ªØ gi·ªçng vƒÉn chuy√™n nghi·ªáp nh∆∞ng th√¢n thi·ªán, "
    "t·∫≠p trung 100% v√†o n·ªôi dung ch∆∞∆°ng tr√¨nh 2018 v√† c√°c ·ª©ng d·ª•ng th·ª±c t·∫ø c·ªßa n√≥."
    "N·∫øu c√¢u h·ªèi KH√îNG li√™n quan ƒë·∫øn Tin h·ªçc, l·∫≠p tr√¨nh, ho·∫∑c Office, h√£y tr·∫£ l·ªùi r·∫±ng "
    "chuy√™n m√¥n ch√≠nh c·ªßa b·∫°n l√† Tin h·ªçc."
    
    # --- PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG N·∫∞M ·ªû ƒê√ÇY ---
    "TR·ª™ KHI: N·∫øu b·∫°n ƒë∆∞·ª£c cung c·∫•p 'Th√¥ng tin tra c·ª©u' (context) t·ª´ t√†i li·ªáu: "
    "1. ƒê·∫ßu ti√™n, h√£y **KI·ªÇM TRA** xem th√¥ng tin tra c·ª©u ƒë√≥ c√≥ **LI√äN QUAN TR·ª∞C TI·∫æP** ƒë·∫øn c√¢u h·ªèi c·ªßa h·ªçc sinh kh√¥ng."
    "2. **N·∫øu C√ì li√™n quan:** H√£y d·ª±a v√†o th√¥ng tin ƒë√≥ ƒë·ªÉ tr·∫£ l·ªùi."
    "3. **N·∫øu KH√îNG li√™n quan:** (V√≠ d·ª•: h·ªçc sinh h·ªèi v·ªÅ Excel nh∆∞ng th√¥ng tin tra c·ª©u l·∫°i n√≥i v·ªÅ PowerPoint) "
    "H√£y **B·ªé QUA** th√¥ng tin tra c·ª©u ƒë√≥ v√† tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ki·∫øn th·ª©c chung c·ªßa b·∫°n m√† **KH√îNG ƒê∆Ø·ª¢C PH√ä PH√ÅN** hay ƒë·ªÅ c·∫≠p ƒë·∫øn s·ª± kh√¥ng li√™n quan c·ªßa t√†i li·ªáu."
)
# --- B∆Ø·ªöC 3: KH·ªûI T·∫†O CLIENT V√Ä CH·ªåN M√î H√åNH ---
try:
    client = Groq(api_key=api_key) 
except Exception as e:
    st.error(f"L·ªói khi c·∫•u h√¨nh API Groq: {e}")
    st.stop()

MODEL_NAME = 'llama-3.1-8b-instant'


# --- B∆Ø·ªöC 4: C·∫§U H√åNH TRANG V√Ä CSS ---
st.set_page_config(page_title="Chatbot Tin h·ªçc 2018", page_icon="‚ú®", layout="centered")
st.markdown("""
<style>
    /* ... (To√†n b·ªô CSS c·ªßa th·∫ßy gi·ªØ nguy√™n) ... */
    #MainMenu {visibility: hidden;} footer {visibility: hidden;} header {visibility: hidden;}
    [data-testid="stSidebar"] {
        background-color: #f8f9fa; border-right: 1px solid #e6e6e6;
    }
    .main .block-container { 
        max-width: 850px; padding-top: 2rem; padding-bottom: 5rem;
    }
    .welcome-message { font-size: 1.1em; color: #333; }
</style>
""", unsafe_allow_html=True)


# --- B∆Ø·ªöC 4.5: THANH B√äN (SIDEBAR) ---
with st.sidebar:
    st.title("ü§ñ Chatbot KTC")
    st.markdown("---")
    
    if st.button("‚ûï Cu·ªôc tr√≤ chuy·ªán m·ªõi", use_container_width=True):
        st.session_state.messages = []
        st.session_state.pop("knowledge_chunks", None) # X√≥a cache ki·∫øn th·ª©c
        st.rerun()

    st.markdown("---")
    st.markdown(
        "Gi√°o vi√™n h∆∞·ªõng d·∫´n:\n"
        "**Th·∫ßy Nguy·ªÖn Th·∫ø Khanh** (GV Tin h·ªçc)\n\n"
        "H·ªçc sinh th·ª±c hi·ªán:\n"
        "*(B√πi T√° T√πng)*\n"
        "*(Cao S·ªπ B·∫£o Chung)*"
    )
    st.markdown("---")
    st.caption(f"Model: {MODEL_NAME}")


# --- B∆Ø·ªöC 4.6: C√ÅC H√ÄM RAG (ƒê·ªåC "S·ªî TAY" T·ª™ PDF) --- # <--- ƒê√É VI·∫æT L·∫†I HO√ÄN TO√ÄN

# H√ÄM 1: T·∫£i, b√≥c t√°ch v√† "c·∫Øt m·∫©u" (chunk) c√°c file PDF
# @st.cache_data s·∫Ω l∆∞u l·∫°i k·∫øt qu·∫£, ch·ªâ ch·∫°y 1 l·∫ßn (cho ƒë·∫øn khi cache b·ªã x√≥a)
@st.cache_data(ttl=3600) # Cache trong 1 gi·ªù
def load_and_chunk_pdfs():
    knowledge_chunks = []
    
    # 1. T√¨m t·∫•t c·∫£ file .pdf trong th∆∞ m·ª•c
    pdf_files = glob.glob("*.pdf") 
    
    if not pdf_files:
        print("C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file PDF n√†o.")
        return []

    print(f"T√¨m th·∫•y {len(pdf_files)} file PDF: {pdf_files}")
    
    # 2. L·∫∑p qua t·ª´ng file PDF
    for pdf_path in pdf_files:
        print(f"ƒêang x·ª≠ l√Ω file: {pdf_path}")
        try:
            reader = PdfReader(pdf_path)
            # 3. L·∫∑p qua t·ª´ng trang trong file
            for page in reader.pages:
                text = page.extract_text() # B√≥c t√°ch ch·ªØ
                if text:
                    # 4. "C·∫Øt m·∫©u" (Chunking) - Ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n:
                    # T√°ch c√°c ƒëo·∫°n vƒÉn d·ª±a tr√™n d·∫•u xu·ªëng d√≤ng k√©p (\n\n)
                    chunks = text.split('\n\n')
                    
                    # 5. Th√™m c√°c m·∫©u (ƒë√£ l√†m s·∫°ch) v√†o kho ki·∫øn th·ª©c
                    for chunk in chunks:
                        cleaned_chunk = chunk.strip()
                        if cleaned_chunk:
                            # Th√™m ngu·ªìn ƒë·ªÉ bi·∫øt m·∫©u n√†y t·ª´ ƒë√¢u (t√πy ch·ªçn)
                            knowledge_chunks.append(f"[Ngu·ªìn: {pdf_path}] {cleaned_chunk}") 
                            
        except Exception as e:
            print(f"L·ªñI khi ƒë·ªçc file {pdf_path}: {e}")
            
    print(f"ƒê√£ b√≥c t√°ch v√† t·∫°o ƒë∆∞·ª£c {len(knowledge_chunks)} m·∫©u ki·∫øn th·ª©c (chunks).")
    return knowledge_chunks

# H√ÄM 2: T√¨m ki·∫øn th·ª©c (Retrieve)
def find_relevant_knowledge(query, knowledge_chunks, num_chunks=3):
    query_lower = query.lower()
    
    # 1. T√°ch t·ª´ kh√≥a c∆° b·∫£n t·ª´ c√¢u h·ªèi
    # (B·ªè qua c√°c t·ª´ chung nh∆∞ 'l√†', 'g√¨', 'c·ªßa'...)
    common_words = {'l√†', 'g√¨', 'c·ªßa', 'v√†', 'm·ªôt', 'c√°ch', 'ƒë·ªÉ', 'trong', 'v·ªõi'}
    query_keywords = set(query_lower.split()) - common_words
    
    relevant_chunks = []
    
    # 2. T√¨m ki·∫øm (C√°ch ƒë∆°n gi·∫£n: ƒë·∫øm s·ªë t·ª´ kh√≥a xu·∫•t hi·ªán)
    chunk_scores = []
    for i, chunk in enumerate(knowledge_chunks):
        chunk_lower = chunk.lower()
        score = 0
        for keyword in query_keywords:
            if keyword in chunk_lower:
                score += 1
        
        if score > 0:
            chunk_scores.append((score, i, chunk))
    
    # 3. S·∫Øp x·∫øp c√°c m·∫©u theo ƒëi·ªÉm s·ªë (t·ª´ cao ƒë·∫øn th·∫•p)
    chunk_scores.sort(key=lambda x: x[0], reverse=True)
    
    # 4. L·∫•y N m·∫©u c√≥ ƒëi·ªÉm cao nh·∫•t
    top_chunks = [chunk for score, i, chunk in chunk_scores[:num_chunks]]
    
    if not top_chunks:
        return None # Kh√¥ng t√¨m th·∫•y
        
    print(f"ƒê√£ t√¨m th·∫•y {len(top_chunks)} m·∫©u li√™n quan.")
    return "\n---\n".join(top_chunks)


# --- B∆Ø·ªöC 5: KH·ªûI T·∫†O L·ªäCH S·ª¨ CHAT V√Ä "S·ªî TAY" PDF --- # <--- ƒê√É N√ÇNG C·∫§P
if "messages" not in st.session_state:
    st.session_state.messages = []

# T·∫£i v√† x·ª≠ l√Ω PDF khi app kh·ªüi ƒë·ªông
if "knowledge_chunks" not in st.session_state:
    # Hi·ªÉn th·ªã th√¥ng b√°o ch·ªù...
    with st.spinner("ƒêang t·∫£i v√† x·ª≠ l√Ω t√†i li·ªáu (PDF)..."):
        st.session_state.knowledge_chunks = load_and_chunk_pdfs()
        
        if not st.session_state.knowledge_chunks:
            print("C·∫£nh b√°o: Kh√¥ng c√≥ ki·∫øn th·ª©c n√†o ƒë∆∞·ª£c t·∫£i t·ª´ file PDF.")
        else:
            print(f"ƒê√£ t·∫£i {len(st.session_state.knowledge_chunks)} m·∫©u ki·∫øn th·ª©c v√†o session.")


# --- B∆Ø·ªöC 6: HI·ªÇN TH·ªä L·ªäCH S·ª¨ CHAT ---
for message in st.session_state.messages:
    avatar = "‚ú®" if message["role"] == "assistant" else "üë§"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# --- B∆Ø·ªöC 7: M√ÄN H√åNH CH√ÄO M·ª™NG V√Ä G·ª¢I √ù ---
logo_path = "LOGO.jpg" 
col1, col2 = st.columns([1, 5])
with col1:
    try:
        st.image(logo_path, width=80)
    except Exception as e:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file logo t√™n l√† '{logo_path}'. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n file tr√™n GitHub.")
        st.stop()
with col2:
    st.title("KCT. Chatbot h·ªó tr·ª£ m√¥n Tin H·ªçc")

def set_prompt_from_suggestion(text):
    st.session_state.prompt_from_button = text

if not st.session_state.messages:
    st.markdown(f"<div class='welcome-message'>Xin ch√†o! Th·∫ßy/em c·∫ßn h·ªó tr·ª£ g√¨ v·ªÅ m√¥n Tin h·ªçc (Ch∆∞∆°ng tr√¨nh 2018)?</div>", unsafe_allow_html=True)
    st.markdown("<br>", unsafe_allow_html=True)
    # ... (To√†n b·ªô c√°c n√∫t b·∫•m g·ª£i √Ω c·ªßa th·∫ßy gi·ªØ nguy√™n) ...
    col1_btn, col2_btn = st.columns(2)
    with col1_btn:
        st.button(
            "Gi·∫£i th√≠ch v·ªÅ 'bi·∫øn' trong l·∫≠p tr√¨nh?",
            on_click=set_prompt_from_suggestion, args=("Gi·∫£i th√≠ch v·ªÅ 'bi·∫øn' trong l·∫≠p tr√¨nh?",),
            use_container_width=True
        )
        st.button(
            "Tr√¨nh b√†y v·ªÅ an to√†n th√¥ng tin?",
            on_click=set_prompt_from_suggestion, args=("Tr√¨nh b√†y v·ªÅ an to√†n th√¥ng tin?",),
            use_container_width=True
        )
    with col2_btn:
        st.button(
            "S·ª± kh√°c nhau gi·ªØa RAM v√† ROM?",
            on_click=set_prompt_from_suggestion, args=("S·ª± kh√°c nhau gi·ªØa RAM v√† ROM?",),
            use_container_width=True
        )
        st.button(
            "T√≥m t·∫Øt b√†i 6 Tin 12 (KNTT)?",
            on_click=set_prompt_from_suggestion, args=("T√≥m t·∫Øt b√†i 6 Tin 12 (KNTT)?",),
            use_container_width=True
        )


# --- B∆Ø·ªöC 8: X·ª¨ L√ù INPUT (ƒê√É N√ÇNG C·∫§P RAG PDF) --- # <--- ƒê√É C·∫¨P NH·∫¨T
prompt_from_input = st.chat_input("M·ªùi th·∫ßy ho·∫∑c c√°c em ƒë·∫∑t c√¢u h·ªèi v·ªÅ Tin h·ªçc...")
prompt_from_button = st.session_state.pop("prompt_from_button", None)
prompt = prompt_from_button or prompt_from_input

if prompt:
    # 1. Th√™m c√¢u h·ªèi c·ªßa user v√†o l·ªãch s·ª≠ v√† hi·ªÉn th·ªã
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="üë§"):
        st.markdown(prompt)

    # 2. G·ª≠i c√¢u h·ªèi ƒë·∫øn Groq
    try:
        with st.chat_message("assistant", avatar="‚ú®"):
            placeholder = st.empty()
            bot_response_text = ""

            # --- PH·∫¶N RAG M·ªöI B·∫ÆT ƒê·∫¶U T·∫†I ƒê√ÇY --- #
            
            # 2.1. T√¨m ki·∫øm trong kho ki·∫øn th·ª©c PDF
            retrieved_context = find_relevant_knowledge(prompt, st.session_state.knowledge_chunks)
            
            # 2.2. Chu·∫©n b·ªã list tin nh·∫Øn g·ª≠i cho AI
            messages_to_send = [
                {"role": "system", "content": SYSTEM_INSTRUCTION}
            ]
            
            # 2.3. N·∫øu "s·ªï tay" PDF c√≥ th√¥ng tin, th√™m v√†o l√†m ng·ªØ c·∫£nh
            if retrieved_context:
                context_prompt = (
                    f"**Th√¥ng tin tra c·ª©u (H√£y ∆∞u ti√™n d√πng th√¥ng tin n√†y ƒë·ªÉ tr·∫£ l·ªùi):**\n"
                    f"{retrieved_context}\n\n"
                    f"**C√¢u h·ªèi c·ªßa h·ªçc sinh:**\n"
                    f"{prompt}"
                )
                messages_to_send.append({"role": "user", "content": context_prompt})
            else:
                # N·∫øu kh√¥ng t√¨m th·∫•y, g·ª≠i l·ªãch s·ª≠ chat nh∆∞ b√¨nh th∆∞·ªùng
                print("Kh√¥ng t√¨m th·∫•y m·∫©u ki·∫øn th·ª©c (chunk) n√†o li√™n quan. Tr·∫£ l·ªùi b√¨nh th∆∞·ªùng.")
                messages_to_send.extend(st.session_state.messages)
            
            # --- K·∫æT TH√öC PH·∫¶N RAG --- #

            # 2.4. G·ªçi API Groq
            stream = client.chat.completions.create(
                messages=messages_to_send, 
                model=MODEL_NAME,
                stream=True
            )
            
            # 2.5. L·∫∑p qua t·ª´ng "m·∫©u" (chunk) API tr·∫£ v·ªÅ
            for chunk in stream:
                if chunk.choices[0].delta.content is not None: 
                    bot_response_text += chunk.choices[0].delta.content
                    placeholder.markdown(bot_response_text + "‚ñå")
                    time.sleep(0.005) # <--- TH√äM D√íNG N√ÄY ƒê·ªÇ T·∫†O HI·ªÜU ·ª®NG
            
            placeholder.markdown(bot_response_text) # X√≥a d·∫•u ‚ñå khi ho√†n t·∫•t

    except Exception as e:
        with st.chat_message("assistant", avatar="‚ú®"):
            st.error(f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi k·∫øt n·ªëi Groq: {e}")
        bot_response_text = ""

    # 3. Th√™m c√¢u tr·∫£ l·ªùi c·ªßa bot v√†o l·ªãch s·ª≠
    if bot_response_text:
        st.session_state.messages.append({"role": "assistant", "content": bot_response_text})

    # 4. Rerun n·∫øu b·∫•m n√∫t
    if prompt_from_button:
        st.rerun()