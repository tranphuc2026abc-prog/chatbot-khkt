import streamlit as st
import os
import glob
import nest_asyncio
import pickle
from groq import Groq

# --- C√ÅC TH∆Ø VI·ªÜN RAG N√ÇNG CAO ---
from llama_parse import LlamaParse  # C√¥ng c·ª• parse PDF x·ªãn nh·∫•t hi·ªán nay
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# Apply asyncio fix cho LlamaParse ch·∫°y tr√™n Streamlit
nest_asyncio.apply()

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
st.set_page_config(
    page_title="Chatbot KTC - Tr·ª£ l√Ω Tin h·ªçc",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
MODEL_NAME = 'llama-3.1-8b-instant'
PDF_DIR = "./PDF_KNOWLEDGE"
CACHE_DIR = "./CACHE_DATA" # N∆°i l∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ƒë·ªÉ kh√¥ng ph·∫£i parse l·∫°i
LOGO_PATH = "LOGO.jpg"

# ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
os.makedirs(PDF_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)

# --- 2. CSS GIAO DI·ªÜN (GI·ªÆ NGUY√äN STYLE C·ª¶A B·∫†N) ---
st.markdown("""
<style>
    .stApp {background-color: #f8f9fa;}
    [data-testid="stSidebar"] {background-color: #ffffff; border-right: 1px solid #e0e0e0;}
    .author-box {background-color: #f0f8ff; border: 1px solid #bae6fd; border-radius: 10px; padding: 15px; margin-top: 15px; color: #0f172a;}
    .author-header {font-weight: bold; color: #0284c7; margin-bottom: 5px; font-size: 0.85rem; text-transform: uppercase; margin-top: 10px;}
    .gradient-text {background: linear-gradient(90deg, #0f4c81, #1cb5e0); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-weight: 800; font-size: 2.5rem; text-align: center;}
    div[data-testid="stChatMessage"]:nth-child(even) {background-color: #ffffff; border: 1px solid #e2e8f0; border-radius: 0px 15px 15px 15px;}
    div[data-testid="stChatMessage"]:nth-child(odd) {background-color: #e0f2fe; border-radius: 15px 0px 15px 15px; border: none;}
    .footer-note {text-align: center; font-size: 0.75rem; color: #94a3b8; margin-top: 30px; border-top: 1px dashed #cbd5e1; padding-top: 10px;}
</style>
""", unsafe_allow_html=True)

# --- 3. X·ª¨ L√ù K·∫æT N·ªêI API ---
try:
    groq_api_key = st.secrets["GROQ_API_KEY"]
    llama_api_key = st.secrets["LLAMA_CLOUD_API_KEY"]
except (KeyError, FileNotFoundError):
    st.error("‚ùå L·ªói: Thi·∫øu API Key trong secrets.toml (C·∫ßn c·∫£ GROQ_API_KEY v√† LLAMA_CLOUD_API_KEY)")
    st.stop()

client = Groq(api_key=groq_api_key)

# --- 4. H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU TH√îNG MINH (THEO ƒê·ªäNH H∆Ø·ªöNG M·ªöI) ---

@st.cache_resource(show_spinner=False)
def load_and_process_data():
    """
    Quy tr√¨nh:
    1. Ki·ªÉm tra xem ƒë√£ c√≥ file Markdown cache ch∆∞a.
    2. N·∫øu ch∆∞a -> D√πng LlamaParse chuy·ªÉn PDF -> Markdown (Gi·ªØ c·∫•u tr√∫c b·∫£ng/h√¨nh).
    3. L∆∞u cache ƒë·ªÉ l·∫ßn sau ch·∫°y nhanh h∆°n.
    4. Chia nh·ªè vƒÉn b·∫£n theo ng·ªØ nghƒ©a (Header splitter).
    5. T·∫°o Vector DB.
    """
    pdf_files = glob.glob(os.path.join(PDF_DIR, "*.pdf"))
    if not pdf_files:
        return None

    all_documents = []
    
    # --- GIAI ƒêO·∫†N 1: PARSING (PDF -> MARKDOWN) ---
    with st.spinner('üîÑ ƒêang s·ªë h√≥a tri th·ª©c SGK (LlamaParse)...'):
        parser = LlamaParse(
            api_key=llama_api_key,
            result_type="markdown",
            verbose=True,
            language="vi",
            gpt4o_mode=True # Ch·∫ø ƒë·ªô th√¥ng minh nh·∫•t ƒë·ªÉ hi·ªÉu layout SGK
        )

        for pdf_path in pdf_files:
            file_name = os.path.basename(pdf_path)
            cache_path = os.path.join(CACHE_DIR, f"{file_name}.md")
            
            markdown_text = ""

            # Ki·ªÉm tra cache
            if os.path.exists(cache_path):
                with open(cache_path, "r", encoding="utf-8") as f:
                    markdown_text = f.read()
            else:
                # N·∫øu ch∆∞a c√≥ cache th√¨ g·ªçi API parse
                try:
                    documents = parser.load_data(pdf_path)
                    markdown_text = "\n".join([doc.text for doc in documents])
                    # L∆∞u cache l·∫°i
                    with open(cache_path, "w", encoding="utf-8") as f:
                        f.write(markdown_text)
                except Exception as e:
                    st.warning(f"Kh√¥ng th·ªÉ ƒë·ªçc file {file_name}: {e}")
                    continue
            
            # T·∫°o document th√¥ ban ƒë·∫ßu
            if markdown_text:
                # Th√™m t√™n ngu·ªìn v√†o ƒë·∫ßu vƒÉn b·∫£n ƒë·ªÉ AI bi·∫øt
                markdown_text = f"Ngu·ªìn t√†i li·ªáu: {file_name}\n\n" + markdown_text
                all_documents.append(Document(page_content=markdown_text, metadata={"source": file_name}))

    if not all_documents:
        return None

    # --- GIAI ƒêO·∫†N 2: CHUNKING (CHIA NH·ªé THEO C·∫§U TR√öC) ---
    with st.spinner('üß† ƒêang t·ªï ch·ª©c l·∫°i ki·∫øn th·ª©c (Markdown Splitting)...'):
        # 1. C·∫Øt theo Header (Ch∆∞∆°ng/B√†i) tr∆∞·ªõc ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        
        md_header_splits = []
        for doc in all_documents:
            splits = markdown_splitter.split_text(doc.page_content)
            for split in splits:
                split.metadata["source"] = doc.metadata["source"] # Copy metadata ngu·ªìn
                md_header_splits.append(split)

        # 2. C·∫Øt m·ªãn l·∫°i n·∫øu ƒëo·∫°n vƒÉn v·∫´n qu√° d√†i (ƒë·∫£m b·∫£o v·ª´a context window)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        final_splits = text_splitter.split_documents(md_header_splits)

    # --- GIAI ƒêO·∫†N 3: EMBEDDING & VECTOR DB ---
    with st.spinner('üíæ ƒêang ghi nh·ªõ v√†o n√£o b·ªô...'):
        # D√πng model Multilingual ƒë·ªÉ hi·ªÉu ti·∫øng Vi·ªát t·ªët h∆°n model c≈©
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        vector_db = FAISS.from_documents(final_splits, embeddings)
        
    return vector_db

# --- KH·ªûI T·∫†O STATE ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n! M√¨nh l√† Chatbot KTC ü§ñ. M√¨nh ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p ƒë·ªÉ ƒë·ªçc SGK ch√≠nh x√°c h∆°n. B·∫°n c·∫ßn h·ªèi g√¨ n√†o?"}]

if "vector_db" not in st.session_state:
    st.session_state.vector_db = load_and_process_data()

# --- 5. SIDEBAR ---
with st.sidebar:
    if os.path.exists(LOGO_PATH):
        st.image(LOGO_PATH, use_container_width=True)
    
    st.markdown("""
        <div style='text-align: center; margin-top: 10px;'>
            <h3 style='color: #0f4c81; margin: 0;'>TR·ª¢ L√ù KTC</h3>
            <p style='font-size: 0.8rem; color: #64748b;'>Powered by LlamaParse & Groq</p>
        </div>
        <hr style="margin: 15px 0;">
    """, unsafe_allow_html=True)
    
    if st.session_state.vector_db:
        st.markdown("üíæ Tr·∫°ng th√°i: <span style='color:green; font-weight:bold'>‚óè S·∫µn s√†ng</span>", unsafe_allow_html=True)
    else:
        st.markdown("üíæ Tr·∫°ng th√°i: <span style='color:red; font-weight:bold'>‚óè Ch∆∞a c√≥ d·ªØ li·ªáu</span>", unsafe_allow_html=True)
        st.info("H√£y b·ªè file PDF v√†o th∆∞ m·ª•c PDF_KNOWLEDGE nh√©.")

    html_info = """
    <div class="author-box">
        <div class="author-header">üè´ S·∫£n ph·∫©m KHKT</div>
        <div class="author-content">NƒÉm h·ªçc 2025 - 2026</div>
        <div class="author-header">üë®‚Äçüè´ GV H∆∞·ªõng D·∫´n</div>
        <div class="author-content">Th·∫ßy Nguy·ªÖn Th·∫ø Khanh</div>
        <div class="author-header">üßë‚Äçüéì Nh√≥m t√°c gi·∫£</div>
        <ul class="author-list">
            <li>B√πi T√° T√πng</li>
            <li>Cao S·ªπ B·∫£o Chung</li>
        </ul>
    </div>
    """
    st.markdown(html_info, unsafe_allow_html=True)
    
    if st.button("üóëÔ∏è X√≥a b·ªô nh·ªõ t·∫°m", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- 6. GIAO DI·ªÜN CH√çNH & X·ª¨ L√ù CHAT ---
col1, col2, col3 = st.columns([1, 8, 1])

with col2:
    st.markdown('<h1 class="gradient-text">CHATBOT H·ªñ TR·ª¢ H·ªåC T·∫¨P KTC</h1>', unsafe_allow_html=True)
    
    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if message["role"] == "user" else "ü§ñ"
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"], unsafe_allow_html=True)

    # Input ng∆∞·ªùi d√πng
    prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y...")

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        # --- LOGIC RAG ---
        context_text = ""
        sources_list = []
        
        if st.session_state.vector_db:
            # T√¨m ki·∫øm 4 ƒëo·∫°n vƒÉn b·∫£n li√™n quan nh·∫•t
            results = st.session_state.vector_db.similarity_search(prompt, k=4)
            for doc in results:
                # L·∫•y metadata header n·∫øu c√≥
                header_info = ""
                if "Header 1" in doc.metadata: header_info += f" > {doc.metadata['Header 1']}"
                if "Header 2" in doc.metadata: header_info += f" > {doc.metadata['Header 2']}"
                
                context_text += f"\n---\n[Ngu·ªìn: {doc.metadata['source']}{header_info}]\nN·ªôi dung: {doc.page_content}\n"
                sources_list.append(f"{doc.metadata['source']}{header_info}")

        # System Prompt ƒë∆∞·ª£c tinh ch·ªânh ƒë·ªÉ tr√≠ch xu·∫•t ch√≠nh x√°c
        SYSTEM_PROMPT = """
        B·∫°n l√† tr·ª£ l√Ω AI gi√°o d·ª•c (KTC Chatbot). Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n D·ªÆ LI·ªÜU ƒê∆Ø·ª¢C CUNG C·∫§P (Context).
        
        QUY T·∫ÆC TR·∫¢ L·ªúI:
        1. CH√çNH X√ÅC: Ch·ªâ d√πng th√¥ng tin trong Context. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i "Xin l·ªói, s√°ch gi√°o khoa kh√¥ng ƒë·ªÅ c·∫≠p v·∫•n ƒë·ªÅ n√†y."
        2. TR√åNH B√ÄY: D√πng Markdown. N·∫øu c√≥ c√¥ng th·ª©c to√°n/tin, h√£y vi·∫øt r√µ r√†ng. N·∫øu c√≥ b·∫£ng bi·ªÉu trong context, h√£y v·∫Ω l·∫°i b·∫£ng.
        3. NG√îN NG·ªÆ: Ti·∫øng Vi·ªát s∆∞ ph·∫°m, d·ªÖ hi·ªÉu, ph√π h·ª£p h·ªçc sinh.
        4. TR√çCH D·∫™N: Lu√¥n nh·∫Øc ƒë·∫øn th√¥ng tin n√†y n·∫±m ·ªü b√†i n√†o/ch∆∞∆°ng n√†o n·∫øu Context c√≥ cung c·∫•p.
        """
        
        final_prompt = f"{SYSTEM_PROMPT}\n\n--- D·ªÆ LI·ªÜU THAM KH·∫¢O T·ª™ SGK ---\n{context_text}\n\n--- C√ÇU H·ªéI H·ªåC SINH ---\n{prompt}"

        with st.chat_message("assistant", avatar="ü§ñ"):
            placeholder = st.empty()
            full_response = ""
            
            try:
                chat_completion = client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": final_prompt}, 
                        {"role": "user", "content": prompt}
                    ],
                    model=MODEL_NAME, 
                    stream=True, 
                    temperature=0.2 # Gi·∫£m nhi·ªát ƒë·ªô ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
                )

                for chunk in chat_completion:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        placeholder.markdown(full_response + "‚ñå")
                
                # Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o
                if sources_list:
                    unique_sources = list(set(sources_list))
                    citation_html = "<div style='margin-top:10px; font-size: 0.85em; color: #666; border-top: 1px solid #ddd; padding-top: 5px;'>üìö <b>Ngu·ªìn SGK tham chi·∫øu:</b><br>" + "<br>".join([f"- <i>{s}</i>" for s in unique_sources]) + "</div>"
                    placeholder.markdown(full_response + "\n\n" + citation_html, unsafe_allow_html=True)
                else:
                    placeholder.markdown(full_response)

                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                st.error(f"L·ªói k·∫øt n·ªëi AI: {e}")

    st.markdown('<div class="footer-note">‚ö†Ô∏è KTC Chatbot h·ªó tr·ª£ h·ªçc t·∫≠p - H√£y ƒë·ªëi chi·∫øu v·ªõi SGK g·ªëc.</div>', unsafe_allow_html=True)