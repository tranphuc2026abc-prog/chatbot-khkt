import os
import glob
import base64
import streamlit as st
import shutil
import pickle
import re
import uuid
import unicodedata 
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Generator
from collections import defaultdict

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply() 
    try:
        from llama_parse import LlamaParse 
    except ImportError:
        LlamaParse = None
    
    # üî• NEW: PyMuPDF for advanced processing
    import fitz  # PyMuPDF
    
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="LOGO.jpg",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    LLM_MODEL = 'llama-3.1-8b-instant'
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"

    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    PROCESSED_MD_DIR = "PROCESSED_MD" 

    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"

    # RAG Parameters
    RETRIEVAL_K = 30       
    FINAL_K = 5
    RERANK_THRESHOLD = 0.45  # Score threshold for filtering
    
    # Synthetic Scoring (Fallback when Reranker fails)
    SYNTHETIC_BASE_SCORE = 0.95
    SYNTHETIC_DECAY = 0.05
    
    # Hybrid Search Weights
    BM25_WEIGHT = 0.4      
    FAISS_WEIGHT = 0.6     

    LLM_TEMPERATURE = 0.0 

# ===============================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER ) 
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown, .stButton, .stTextInput, .stChatInput {
                font-family: 'Inter', sans-serif !important;
            }
            section[data-testid="stSidebar"] {
                background-color: #f8f9fa; border-right: 1px solid #e9ecef;
            }
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                margin-bottom: 5px; text-align: center; text-transform: uppercase;
            }
            .project-sub {
                font-size: 0.8rem; color: #6c757d; text-align: center;
                margin-bottom: 15px; font-style: italic;
            }
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 8px 20px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0;
                font-size: 2.2rem; letter-spacing: -0.5px;
            }
            .header-left p {
                color: #e0fbfc; margin: 5px 0 0 0; font-size: 1rem; opacity: 0.9;
            }
            .header-right img {
                border-radius: 50%; border: 3px solid rgba(255,255,255,0.3);
                box-shadow: 0 4px 10px rgba(0,0,0,0.2); width: 100px; height: 100px;
                object-fit: cover;
            }
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
            [data-testid="stChatMessageContent"]:has(+ [data-testid="stChatMessageAvatar"]) {
                background: #e3f2fd; color: #0d47a1;
            }
            [data-testid="stChatMessageContent"]:not(:has(+ [data-testid="stChatMessageAvatar"])) {
                background: white; border: 1px solid #e9ecef;
                border-left: 5px solid #00b4d8;
            }
            
            /* Evidence Card Styles */
            .evidence-card {
                background: #f8f9fa;
                border-left: 4px solid #0077b6;
                padding: 12px 15px;
                margin-bottom: 10px;
                border-radius: 8px;
                font-size: 0.9rem;
            }
            .evidence-header {
                font-weight: 700;
                color: #023e8a;
                margin-bottom: 5px;
                display: flex;
                align-items: center;
                flex-wrap: wrap;
                gap: 8px;
            }
            .evidence-confidence {
                display: inline-block;
                background: linear-gradient(135deg, #0077b6, #00b4d8);
                color: white;
                padding: 3px 10px;
                border-radius: 12px;
                font-size: 0.8rem;
                font-weight: 600;
            }
            .evidence-badge {
                display: inline-block;
                background: #e9ecef;
                color: #495057;
                padding: 3px 8px;
                border-radius: 10px;
                font-size: 0.75rem;
                font-weight: 600;
            }
            .evidence-context {
                color: #495057;
                font-size: 0.85rem;
                margin-top: 5px;
                font-style: italic;
            }
            
            div.stButton > button {
                border-radius: 8px; background-color: white; color: #0077b6;
                border: 1px solid #90e0ef; transition: all 0.2s;
            }
            div.stButton > button:hover {
                background-color: #0077b6; color: white;
                border-color: #0077b6; box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            }
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div class="project-sub">S·∫£n ph·∫©m d·ª± thi KHKT c·∫•p T·ªânh</div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <div style="display: flex; justify-content: space-between;">
                        <span style="font-weight: 600; color: #555;">T√°c gi·∫£:</span>
                        <span style="text-align: right; color: #222;"><b>B√πi T√° T√πng</b><br><b>Cao S·ªπ B·∫£o Chung</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">GVHD:</span>
                        <span style="text-align: right; color: #222;">Th·∫ßy <b>Nguy·ªÖn Th·∫ø Khanh</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">NƒÉm h·ªçc:</span>
                        <span style="text-align: right; color: #222;"><b>2025 - 2026</b></span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                if os.path.exists(AppConfig.PROCESSED_MD_DIR):
                    shutil.rmtree(AppConfig.PROCESSED_MD_DIR)
                st.session_state.pop('retriever_engine', None)
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" alt="Logo">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p style="font-size: 1.1rem; margin-top: 5px;">H·ªçc Tin d·ªÖ d√†ng - Thao t√°c v·ªØng v√†ng</p>
            </div>
            <div class="header-right">
                {img_html}
            </div>
        </div>
        """, unsafe_allow_html=True)

# ============================================================
# üî• ADVANCED PDF PROCESSOR - INTEGRATED MODULE
# ============================================================

class AdvancedPDFProcessor:
    """
    Advanced processor for Vietnamese textbook PDFs with hierarchical structure.
    Implements context-aware chunking with proper metadata tracking.
    
    This replaces the naive RecursiveCharacterTextSplitter approach.
    """
    
    # Noise patterns to filter out
    NOISE_PATTERNS = [
        r'K·∫æT\s+N·ªêI\s+TRI\s+TH·ª®C\s+V·ªöI\s+CU·ªòC\s+S·ªêNG',
        r'TIN\s+H·ªåC\s+\d+',
        r'CH∆Ø∆†NG\s+TR√åNH\s+GI√ÅO\s+D·ª§C',
        r'PH√ÇN\s+PH·ªêI\s+CH∆Ø∆†NG\s+TR√åNH',
        r'^\s*\d+\s*$',  # Isolated page numbers
    ]
    
    # Structural patterns for Vietnamese textbooks
    TOPIC_PATTERN = re.compile(
        r'(?:^|\n)\s*CH·ª¶\s+ƒê·ªÄ\s+(\d+)[\.:\s]*(.*?)(?:\n|$)',
        re.IGNORECASE | re.MULTILINE
    )
    
    LESSON_PATTERN = re.compile(
        r'(?:^|\n)\s*B√ÄI\s+(\d+)[\.:\s]*(.*?)(?:\n|$)',
        re.IGNORECASE | re.MULTILINE
    )
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """Normalize Vietnamese text (NFC normalization, whitespace cleanup)."""
        text = unicodedata.normalize('NFC', text)
        text = text.replace('\xa0', ' ').replace('\u200b', '')
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        return text.strip()
    
    @staticmethod
    def is_noise(text: str) -> bool:
        """Check if a text line is noise (header/footer/page number)."""
        text_clean = text.strip()
        
        if len(text_clean) < 3:
            return True
        
        for pattern in AdvancedPDFProcessor.NOISE_PATTERNS:
            if re.search(pattern, text_clean, re.IGNORECASE):
                return True
        
        if text_clean.isdigit() and len(text_clean) <= 3:
            return True
        
        return False
    
    @staticmethod
    def extract_page_text(page) -> Tuple[str, List[str]]:
        """Extract clean text from a PDF page, filtering noise."""
        text = page.get_text()
        text = AdvancedPDFProcessor.normalize_text(text)
        
        lines = text.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            if line and not AdvancedPDFProcessor.is_noise(line):
                clean_lines.append(line)
        
        full_text = '\n'.join(clean_lines)
        return full_text, clean_lines
    
    @staticmethod
    def detect_topic(text: str) -> Optional[str]:
        """Detect 'Ch·ªß ƒë·ªÅ' (Topic/Chapter) from text."""
        match = AdvancedPDFProcessor.TOPIC_PATTERN.search(text)
        if match:
            topic_num = match.group(1).strip()
            topic_name = match.group(2).strip()
            return f"Ch·ªß ƒë·ªÅ {topic_num}. {topic_name}"
        return None
    
    @staticmethod
    def detect_lesson(text: str) -> Optional[str]:
        """Detect 'B√†i' (Lesson) from text."""
        match = AdvancedPDFProcessor.LESSON_PATTERN.search(text)
        if match:
            lesson_num = match.group(1).strip()
            lesson_name = match.group(2).strip()
            return f"B√†i {lesson_num}. {lesson_name}"
        return None
    
    @staticmethod
    def split_into_semantic_chunks(text: str, max_chunk_size: int = 1000) -> List[str]:
        """Split text into semantic chunks respecting paragraph boundaries."""
        if len(text) <= max_chunk_size:
            return [text]
        
        paragraphs = re.split(r'\n\n+', text)
        chunks = []
        current_chunk = []
        current_length = 0
        
        for para in paragraphs:
            para_len = len(para)
            
            if para_len > max_chunk_size:
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                    current_chunk = []
                    current_length = 0
                
                sentences = re.split(r'([.!?]+\s+)', para)
                temp_chunk = ""
                for sent in sentences:
                    if len(temp_chunk) + len(sent) > max_chunk_size and temp_chunk:
                        chunks.append(temp_chunk.strip())
                        temp_chunk = sent
                    else:
                        temp_chunk += sent
                
                if temp_chunk.strip():
                    chunks.append(temp_chunk.strip())
                    
            elif current_length + para_len + 2 > max_chunk_size:
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_length = para_len
            else:
                current_chunk.append(para)
                current_length += para_len + 2
        
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    @staticmethod
    def process_pdf_advanced(pdf_path: str, chunk_size: int = 1000, overlap: int = 100) -> List[Document]:
        """
        üî• MAIN PROCESSING FUNCTION: Extract PDF with context-aware hierarchical chunking.
        
        Algorithm:
        1. Iterate through all PDF pages
        2. Extract and clean text from each page
        3. Maintain state machine for current topic/lesson context
        4. Detect structural changes (new topic, new lesson)
        5. Create chunks with proper metadata enrichment
        
        Returns:
            List of LangChain Document objects with enriched metadata
        """
        doc = fitz.open(pdf_path)
        source_name = os.path.basename(pdf_path)
        documents = []
        
        # State machine variables
        current_topic = None
        current_lesson = None
        content_buffer = []
        buffer_page_start = 0
        
        print(f"üìö Processing: {source_name}")
        print(f"üìÑ Total pages: {len(doc)}")
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            page_text, lines = AdvancedPDFProcessor.extract_page_text(page)
            
            if not page_text.strip():
                continue
            
            # Detect structural changes on this page
            detected_topic = AdvancedPDFProcessor.detect_topic(page_text)
            detected_lesson = AdvancedPDFProcessor.detect_lesson(page_text)
            
            # STATE TRANSITION: New Topic detected
            if detected_topic:
                if content_buffer:
                    AdvancedPDFProcessor._commit_buffer_to_documents(
                        documents, content_buffer, current_topic, current_lesson,
                        buffer_page_start, page_num - 1, source_name, chunk_size, overlap
                    )
                    content_buffer = []
                
                current_topic = detected_topic
                current_lesson = None
                buffer_page_start = page_num
                print(f"  üìå Page {page_num + 1}: Detected {current_topic}")
            
            # STATE TRANSITION: New Lesson detected
            if detected_lesson:
                if content_buffer:
                    AdvancedPDFProcessor._commit_buffer_to_documents(
                        documents, content_buffer, current_topic, current_lesson,
                        buffer_page_start, page_num - 1, source_name, chunk_size, overlap
                    )
                    content_buffer = []
                
                current_lesson = detected_lesson
                buffer_page_start = page_num
                print(f"    üìñ Page {page_num + 1}: Detected {current_lesson}")
            
            content_buffer.append({'text': page_text, 'page': page_num})
        
        # Commit remaining buffer
        if content_buffer:
            AdvancedPDFProcessor._commit_buffer_to_documents(
                documents, content_buffer, current_topic, current_lesson,
                buffer_page_start, len(doc) - 1, source_name, chunk_size, overlap
            )
        
        doc.close()
        print(f"‚úÖ Generated {len(documents)} context-aware chunks")
        return documents
    
    @staticmethod
    def _commit_buffer_to_documents(
        documents: List[Document],
        buffer: List[Dict],
        topic: Optional[str],
        lesson: Optional[str],
        page_start: int,
        page_end: int,
        source_name: str,
        chunk_size: int,
        overlap: int
    ):
        """Convert accumulated buffer into Document objects with metadata."""
        if not buffer:
            return
        
        full_text = '\n\n'.join([item['text'] for item in buffer])
        representative_page = page_start + (page_end - page_start) // 2
        
        chunks = AdvancedPDFProcessor.split_into_semantic_chunks(full_text, chunk_size)
        
        # Create overlapping chunks
        final_chunks = []
        for i, chunk in enumerate(chunks):
            if i > 0 and overlap > 0:
                prev_chunk = chunks[i - 1]
                overlap_text = prev_chunk[-overlap:] if len(prev_chunk) > overlap else prev_chunk
                chunk = overlap_text + '\n' + chunk
            final_chunks.append(chunk)
        
        # Create Document objects
        for chunk_idx, chunk_text in enumerate(final_chunks):
            metadata = {
                'source': source_name,
                'page': representative_page + 1,  # 1-indexed
                'chapter': topic if topic else 'N·ªôi dung chung',
                'lesson': lesson if lesson else 'Ph·∫ßn gi·ªõi thi·ªáu',
                'chunk_index': chunk_idx,
                'total_chunks': len(final_chunks),
                'page_range': f"{page_start + 1}-{page_end + 1}"
            }
            
            doc = Document(page_content=chunk_text.strip(), metadata=metadata)
            documents.append(doc)

# ==================================
# 3. LOGIC BACKEND - ROBUST RAG ENGINE
# ==================================

class RAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            if not api_key:
                return None
            return Groq(api_key=api_key)
        except Exception:
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        try:
            return HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            st.error(f"L·ªói t·∫£i Embedding: {e}")
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except Exception as e:
            return None

    @staticmethod
    def _detect_grade(filename: str) -> str:
        filename = filename.lower()
        if "10" in filename: return "10"
        if "11" in filename: return "11"
        if "12" in filename: return "12"
        return "general"

    @staticmethod
    def _read_and_process_files(pdf_dir: str) -> List[Document]:
        """
        üî• UPGRADED: Uses advanced context-aware PDF processing.
        
        This method now directly calls AdvancedPDFProcessor which implements:
        - Hierarchical structure detection (Ch·ªß ƒë·ªÅ, B√†i)
        - State machine for context tracking
        - Noise reduction (headers, footers, page numbers)
        - Semantic chunking at paragraph boundaries
        - Full metadata enrichment (chapter, lesson, page)
        """
        if not os.path.exists(pdf_dir):
            os.makedirs(pdf_dir, exist_ok=True)
            return []
        
        pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
        all_chunks: List[Document] = []
        status_text = st.empty()

        if not pdf_files:
            st.warning(f"‚ö†Ô∏è Th∆∞ m·ª•c {pdf_dir} ƒëang tr·ªëng. Vui l√≤ng b·ªè file PDF SGK v√†o.")
            return []

        for file_path in pdf_files:
            source_file = os.path.basename(file_path)
            status_text.text(f"üß† ƒêang x·ª≠ l√Ω c·∫•u tr√∫c tri th·ª©c n√¢ng cao: {source_file}...")
            
            try:
                # üî• Use advanced processor
                file_chunks = AdvancedPDFProcessor.process_pdf_advanced(
                    pdf_path=file_path,
                    chunk_size=1000,
                    overlap=100
                )
                
                if file_chunks:
                    all_chunks.extend(file_chunks)
                    print(f"‚úÖ {source_file}: {len(file_chunks)} chunks created with full metadata")
                else:
                    print(f"‚ö†Ô∏è File {source_file} kh√¥ng t·∫°o ƒë∆∞·ª£c chunk n√†o.")
                    
            except Exception as e:
                st.error(f"‚ùå L·ªói x·ª≠ l√Ω file {source_file}: {str(e)}")
                print(f"Error details: {e}")
                import traceback
                traceback.print_exc()
                
        status_text.empty()
        print(f"üìä Total chunks created: {len(all_chunks)}")
        return all_chunks

    @staticmethod
    def build_hybrid_retriever(embeddings):
        if not embeddings: return None

        vector_db = None
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                vector_db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            except Exception: pass

        if not vector_db:
            chunk_docs = RAGEngine._read_and_process_files(AppConfig.PDF_DIR)
            
            if not chunk_docs:
                st.error(f"Kh√¥ng t·∫°o ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ {AppConfig.PDF_DIR}. H√£y ki·ªÉm tra: 1. C√≥ file PDF kh√¥ng? 2. File c√≥ text kh√¥ng (hay l√† ·∫£nh scan)?")
                return None
            
            vector_db = FAISS.from_documents(chunk_docs, embeddings)
            vector_db.save_local(AppConfig.VECTOR_DB_PATH)

        try:
            docstore_docs = list(vector_db.docstore._dict.values())
            bm25_k = min(AppConfig.RETRIEVAL_K, len(docstore_docs))
            
            if bm25_k > 0:
                bm25_retriever = BM25Retriever.from_documents(docstore_docs)
                bm25_retriever.k = bm25_k

                faiss_retriever = vector_db.as_retriever(
                    search_type="mmr",
                    search_kwargs={"k": AppConfig.RETRIEVAL_K, "lambda_mult": 0.5}
                )

                ensemble_retriever = EnsembleRetriever(
                    retrievers=[bm25_retriever, faiss_retriever],
                    weights=[AppConfig.BM25_WEIGHT, AppConfig.FAISS_WEIGHT]
                )
                return ensemble_retriever
            else:
                return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})
        except Exception as e:
            print(f"L·ªói build retriever: {e}. Fallback v·ªÅ FAISS th∆∞·ªùng.")
            return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})
    
    @staticmethod
    def _sanitize_output(text: str) -> str:
        cjk_pattern = re.compile(r'[\u4e00-\u9fff\u3400-\u4dbf\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af]+')
        text = cjk_pattern.sub("", text)
        
        hallucination_pattern = re.compile(r'\[(ID|Ngu·ªìn|Source|Tr√≠ch d·∫´n|T√†i li·ªáu).*?\]', re.IGNORECASE)
        text = hallucination_pattern.sub("", text)
        
        leakage_pattern = re.compile(r'^(H·ªá th·ªëng|Chatbot|Ph·∫ßn n√†y) (t·ª± ƒë·ªông|s·∫Ω|ƒë√£) (g·∫Øn|th√™m|tr√≠ch d·∫´n).*', re.IGNORECASE | re.MULTILINE)
        text = leakage_pattern.sub("", text)
        
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line_clean = line.strip().lower()
            if line_clean.startswith(('ngu·ªìn:', 'source:', 'tr√≠ch d·∫´n:', 't√†i li·ªáu tham kh·∫£o:')):
                continue
            cleaned_lines.append(line)
        
        return "\n".join(cleaned_lines).strip()

    @staticmethod
    def _format_chat_history(messages: List[Dict]) -> str:
        """Format chat history for context injection"""
        formatted = []
        for msg in messages[-6:]:  # Last 3 turns (6 messages)
            role = "H·ªçc sinh" if msg["role"] == "user" else "Tr·ª£ l√Ω"
            content = re.sub(r'<[^>]+>', '', msg["content"])
            formatted.append(f"{role}: {content[:200]}")
        return "\n".join(formatted)

    @staticmethod
    def generate_response(client, retriever, query: str, chat_history: List[Dict]) -> Tuple[str, List[Tuple[Document, float]]]:
        if not client or not retriever:
            return "‚ùå H·ªá th·ªëng ch∆∞a s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra API Key v√† d·ªØ li·ªáu SGK.", []

        # --- T·∫¶NG 1: RETRIEVAL ---
        try:
            raw_docs = retriever.invoke(query)
            if not raw_docs:
                return "üîç Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong SGK.", []
        except Exception as e:
            return f"L·ªói truy v·∫•n d·ªØ li·ªáu: {str(e)}", []

        # --- T·∫¶NG 2: RERANKING ---
        reranker = RAGEngine.load_reranker()
        scored_docs = []

        if reranker:
            try:
                passages = [
                    {"id": idx, "text": doc.page_content, "meta": doc.metadata}
                    for idx, doc in enumerate(raw_docs)
                ]
                rerank_req = RerankRequest(query=query, passages=passages)
                rerank_results = reranker.rerank(rerank_req)
                
                scored_docs = [
                    (raw_docs[res["id"]], res["score"])
                    for res in rerank_results[:AppConfig.FINAL_K]
                    if res["score"] >= AppConfig.RERANK_THRESHOLD
                ]
            except Exception as e:
                print(f"‚ö†Ô∏è Reranker failed: {e}. Using synthetic scores.")
                reranker = None
        
        if not reranker or not scored_docs:
            scored_docs = [
                (doc, AppConfig.SYNTHETIC_BASE_SCORE - (i * AppConfig.SYNTHETIC_DECAY))
                for i, doc in enumerate(raw_docs[:AppConfig.FINAL_K])
            ]

        if not scored_docs:
            return "üîç Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong SGK.", []

        # --- T·∫¶NG 3: CONTEXT BUILDING ---
        context_parts = []
        for doc, _ in scored_docs:
             context_parts.append(
                f"--- BEGIN DATA ---\n{doc.page_content}\n--- END DATA ---"
            )

        full_context = "\n".join(context_parts)
        history_context = RAGEngine._format_chat_history(chat_history)

        # --- T·∫¶NG 4: PROMPT WITH MEMORY ---
        system_prompt = f"""B·∫°n l√† KTC Chatbot, tr·ª£ l√Ω ·∫£o AI h·ªó tr·ª£ h·ªçc t·∫≠p Tin h·ªçc tr∆∞·ªùng Ph·∫°m Ki·ªát.
Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa h·ªçc sinh d·ª±a tr√™n th√¥ng tin trong [CONTEXT] v√† [L·ªäCH S·ª¨ H·ªòI THO·∫†I].

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin trong [CONTEXT].
2. S·ª≠ d·ª•ng [L·ªäCH S·ª¨ H·ªòI THO·∫†I] ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh (v√≠ d·ª•: "cho t√¥i v√≠ d·ª• v·ªÅ c√°i ƒë√≥" ‚Üí bi·∫øt "c√°i ƒë√≥" l√† g√¨).
3. KH√îNG t·ª± vi·∫øt ngu·ªìn tham kh·∫£o gi·∫£.
4. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s∆∞ ph·∫°m, d·ªÖ hi·ªÉu cho h·ªçc sinh ph·ªï th√¥ng.

[L·ªäCH S·ª¨ H·ªòI THO·∫†I]
{history_context}

[CONTEXT]
{full_context}
"""
        
        try:
            completion = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                stream=False,
                temperature=AppConfig.LLM_TEMPERATURE,
                max_tokens=1500
            )
            raw_response = completion.choices[0].message.content

            if "NO_INFO" in raw_response or not raw_response.strip():
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong SGK hi·ªán c√≥.", []

            cleaned_response = RAGEngine._sanitize_output(raw_response)
            return cleaned_response, scored_docs

        except Exception as e:
            return f"L·ªói x·ª≠ l√Ω h·ªá th·ªëng: {str(e)}", []

# ===================
# 4. MAIN APPLICATION
# ===================

def deduplicate_evidence(evidence_docs: List[Tuple[Document, float]]) -> List[Dict]:
    """
    üî• CRITICAL FIX: Group evidence by unique lesson, show highest score + count
    Returns: [{"source": ..., "chapter": ..., "lesson": ..., "max_score": ..., "count": ...}]
    """
    lesson_groups = defaultdict(lambda: {"docs": [], "scores": []})
    
    for doc, score in evidence_docs:
        src = doc.metadata.get('source', 'Unknown')
        chapter = doc.metadata.get('chapter', '')
        lesson = doc.metadata.get('lesson', '')
        
        # Create unique key: source + chapter + lesson
        key = f"{src}|||{chapter}|||{lesson}"
        lesson_groups[key]["docs"].append(doc)
        lesson_groups[key]["scores"].append(score)
    
    # Build deduplicated list
    deduplicated = []
    for key, data in lesson_groups.items():
        src, chapter, lesson = key.split("|||")
        max_score = max(data["scores"])
        count = len(data["docs"])
        
        deduplicated.append({
            "source": src,
            "chapter": chapter,
            "lesson": lesson,
            "max_score": max_score,
            "count": count
        })
    
    # Sort by score descending
    deduplicated.sort(key=lambda x: x["max_score"], reverse=True)
    return deduplicated

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! KTC Chatbot s·∫µn s√†ng h·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c SGK Tin h·ªçc."}]

    groq_client = RAGEngine.load_groq_client()

    if "retriever_engine" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c s·ªë (Advanced Context-Aware Processing)..."):
            embeddings = RAGEngine.load_embedding_model()
            st.session_state.retriever_engine = RAGEngine.build_hybrid_retriever(embeddings)
            if st.session_state.retriever_engine:
                st.toast("‚úÖ D·ªØ li·ªáu SGK ƒë√£ s·∫µn s√†ng!", icon="üìö")

    # Display chat history
    for msg in st.session_state.messages:
        bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else bot_avatar
        with st.chat_message(msg["role"], avatar=avatar):
            if msg["role"] == "assistant" and "evidence" in msg:
                st.markdown(msg["content"])
                
                # üî• Re-render deduplicated evidence for history
                if msg["evidence"]:
                    deduplicated = deduplicate_evidence(msg["evidence"])
                    with st.expander("üìö Ki·ªÉm ch·ª©ng ngu·ªìn g·ªëc (Evidence)", expanded=False):
                        for item in deduplicated:
                            src = item["source"].replace('.pdf', '').replace('_', ' ')
                            topic = item["chapter"]
                            lesson = item["lesson"]
                            confidence_pct = int(item["max_score"] * 100)
                            count = item["count"]
                            
                            count_badge = f'<span class="evidence-badge">üîç {count} ƒëo·∫°n li√™n quan</span>' if count > 1 else ''
                            
                            st.markdown(f"""
                            <div class="evidence-card">
                                <div class="evidence-header">
                                    üìñ {src}
                                    <span class="evidence-confidence">ƒê·ªô tin c·∫≠y: {confidence_pct}%</span>
                                    {count_badge}
                                </div>
                                <div class="evidence-context">‚ûú {topic} ‚ûú {lesson}</div>
                            </div>
                            """, unsafe_allow_html=True)
            else:
                st.markdown(msg["content"])

    user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi h·ªçc t·∫≠p...")
    
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar=AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"):
            response_placeholder = st.empty()
            
            # Pass chat history for context
            response_text, evidence_docs = RAGEngine.generate_response(
                groq_client,
                st.session_state.retriever_engine,
                user_input,
                st.session_state.messages[:-1]  # Exclude the just-added user message
            )

            # Stream simulation for better UX
            displayed = ""
            for char in response_text:
                displayed += char
                response_placeholder.markdown(displayed + "‚ñå")
            
            response_placeholder.markdown(response_text)

            # üî• Display DEDUPLICATED evidence in expander
            if evidence_docs:
                deduplicated = deduplicate_evidence(evidence_docs)
                with st.expander("üìö Ki·ªÉm ch·ª©ng ngu·ªìn g·ªëc (Evidence)", expanded=False):
                    for item in deduplicated:
                        src = item["source"].replace('.pdf', '').replace('_', ' ')
                        topic = item["chapter"]
                        lesson = item["lesson"]
                        confidence_pct = int(item["max_score"] * 100)
                        count = item["count"]
                        
                        count_badge = f'<span class="evidence-badge">üîç {count} ƒëo·∫°n li√™n quan</span>' if count > 1 else ''
                        
                        st.markdown(f"""
                        <div class="evidence-card">
                            <div class="evidence-header">
                                üìñ {src}
                                <span class="evidence-confidence">ƒê·ªô tin c·∫≠y: {confidence_pct}%</span>
                                {count_badge}
                            </div>
                            <div class="evidence-context">‚ûú {topic} ‚ûú {lesson}</div>
                        </div>
                        """, unsafe_allow_html=True)

            # Store evidence with message for history re-rendering
            st.session_state.messages.append({
                "role": "assistant", 
                "content": response_text,
                "evidence": evidence_docs
            })

if __name__ == "__main__":
    main()