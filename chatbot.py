import os
import glob
import base64
import streamlit as st
import shutil
import re
import uuid
import time
from typing import List, Generator

# --- Imports & Error Handling ---
try:
    import nest_asyncio
    nest_asyncio.apply() # Fix l·ªói loop c·ªßa LlamaParse
    from llama_parse import LlamaParse 
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    LLM_MODEL = 'llama-3.1-8b-instant' # T·ªëc ƒë·ªô cao, ph√π h·ª£p chatbot realtime
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"

    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    PROCESSED_MD_DIR = "PROCESSED_MD" 

    # Assets (N·∫øu kh√¥ng c√≥ file ·∫£nh, h·ªá th·ªëng s·∫Ω d√πng icon m·∫∑c ƒë·ªãnh)
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"

    # RAG Parameters
    RETRIEVAL_K = 20       # L·∫•y r·ªông ƒë·ªÉ l·ªçc
    FINAL_K = 4            # Ch·ªâ ƒë∆∞a 4 context t·ªët nh·∫•t v√†o LLM
    
    # Hybrid Search Weights
    BM25_WEIGHT = 0.4      
    FAISS_WEIGHT = 0.6     

    LLM_TEMPERATURE = 0.3 # Th·∫•p ƒë·ªÉ ·ªïn ƒë·ªãnh, tr√°nh "ch√©m gi√≥"

# ===============================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER) 
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown {
                font-family: 'Inter', sans-serif !important;
            }
            /* Giao di·ªán Header */
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 4px 15px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0; font-size: 2rem;
            }
            .header-left p { color: #e0fbfc; margin: 5px 0 0 0; }
            
            /* Giao di·ªán Project Card b√™n Sidebar */
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                text-align: center; text-transform: uppercase; margin-bottom: 10px;
            }

            /* Badge hi·ªÉn th·ªã ngu·ªìn */
            .source-badge {
                display: inline-flex; align-items: center;
                padding: 4px 10px; border-radius: 20px;
                font-size: 0.75rem; font-weight: 600; color: white;
                margin-right: 8px; margin-top: 8px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                transition: transform 0.2s;
            }
            .source-badge:hover { transform: translateY(-2px); }
            
            /* Chat Message */
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 1px 3px rgba(0,0,0,0.05);
            }
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div style="font-size: 0.85rem; color: #555; text-align: center;">
                    <i>Tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ h·ªçc t·∫≠p m√¥n Tin h·ªçc<br>theo ƒë·ªãnh h∆∞·ªõng CT GDPT 2018</i>
                </div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <b>üë®‚Äçüíª T√°c gi·∫£:</b> B√πi T√° T√πng - Cao S·ªπ B·∫£o Chung<br>
                    <b>üë®‚Äçüè´ GVHD:</b> Th·∫ßy Nguy·ªÖn Th·∫ø Khanh
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu SGK", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                st.session_state.pop('retriever_engine', None)
                st.toast("ƒê√£ x√≥a cache d·ªØ li·ªáu. Vui l√≤ng reload l·∫°i trang!", icon="‚úÖ")
                time.sleep(1)
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" style="width:100px; height:100px; border-radius:50%; border:3px solid rgba(255,255,255,0.3); box-shadow:0 4px 10px rgba(0,0,0,0.2); object-fit:cover;">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p>H·ªèi ƒë√°p Tin h·ªçc - Chu·∫©n ki·∫øn th·ª©c SGK K·∫øt n·ªëi tri th·ª©c</p>
            </div>
            <div class="header-right">
                {img_html}
            </div>
        </div>
        """, unsafe_allow_html=True)

# ==================================
# 3. LOGIC BACKEND (RAG ENGINE)
# ==================================

class RAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            if not api_key: return None
            return Groq(api_key=api_key)
        except Exception: return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        return HuggingFaceEmbeddings(
            model_name=AppConfig.EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except: return None

    # --- X·ª≠ l√Ω t√°ch ƒëo·∫°n vƒÉn b·∫£n (Chunking) theo c·∫•u tr√∫c SGK ---
    @staticmethod
    def _structural_chunking(text: str, source_meta: dict) -> List[Document]:
        lines = text.split('\n')
        chunks = []
        
        current_context = {"chapter": "M·ªü ƒë·∫ßu", "lesson": "T·ªïng quan"}
        buffer = []

        # Regex ƒë∆°n gi·∫£n h√≥a ƒë·ªÉ b·∫Øt ti√™u ƒë·ªÅ
        p_chapter = re.compile(r'^(CH∆Ø∆†NG|Ch∆∞∆°ng)\s+[0-9IVX]+', re.IGNORECASE)
        p_lesson = re.compile(r'^(B√ÄI|B√†i)\s+[0-9]+', re.IGNORECASE)

        def commit_chunk(buf, meta, ctx):
            content = "\n".join(buf).strip()
            if len(content) > 50:
                new_meta = meta.copy()
                new_meta.update({
                    "chunk_id": str(uuid.uuid4())[:8],
                    "chapter": ctx["chapter"],
                    "lesson": ctx["lesson"]
                })
                chunks.append(Document(page_content=content, metadata=new_meta))

        for line in lines:
            line = line.strip()
            if not line: continue
            
            if p_chapter.match(line):
                commit_chunk(buffer, source_meta, current_context)
                buffer = [line]
                current_context["chapter"] = line
            elif p_lesson.match(line):
                commit_chunk(buffer, source_meta, current_context)
                buffer = [line]
                current_context["lesson"] = line
            else:
                buffer.append(line)
        
        commit_chunk(buffer, source_meta, current_context)
        return chunks

    @staticmethod
    def _parse_pdf_with_llama(file_path: str) -> str:
        # Check cache Markdown ƒë√£ x·ª≠ l√Ω ch∆∞a ƒë·ªÉ ƒë·ª° t·ªën API
        os.makedirs(AppConfig.PROCESSED_MD_DIR, exist_ok=True)
        file_name = os.path.basename(file_path)
        md_file_path = os.path.join(AppConfig.PROCESSED_MD_DIR, f"{file_name}.md")
        
        if os.path.exists(md_file_path):
            with open(md_file_path, "r", encoding="utf-8") as f:
                return f.read()
        
        # N·∫øu ch∆∞a c√≥ cache th√¨ g·ªçi LlamaParse
        key = st.secrets.get("LLAMA_CLOUD_API_KEY")
        if not key: return ""

        try:
            parser = LlamaParse(api_key=key, result_type="markdown", language="vi")
            docs = parser.load_data(file_path)
            if docs:
                with open(md_file_path, "w", encoding="utf-8") as f:
                    f.write(docs[0].text)
                return docs[0].text
        except: pass
        return ""

    @staticmethod
    def build_hybrid_retriever(embeddings):
        if not embeddings: return None

        # Load VectorDB t·ª´ ƒëƒ©a n·∫øu c√≥
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
                return db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})
            except: pass

        # N·∫øu ch∆∞a c√≥, qu√©t th∆∞ m·ª•c PDF ƒë·ªÉ t·∫°o m·ªõi
        if not os.path.exists(AppConfig.PDF_DIR):
            os.makedirs(AppConfig.PDF_DIR)
            return None
        
        files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf"))
        if not files: return None
        
        all_chunks = []
        progress_text = st.empty()
        
        for f in files:
            progress_text.text(f"ƒêang s·ªë h√≥a tri th·ª©c: {os.path.basename(f)}...")
            txt = RAGEngine._parse_pdf_with_llama(f)
            if txt:
                chunks = RAGEngine._structural_chunking(txt, {"source": os.path.basename(f)})
                all_chunks.extend(chunks)
        
        progress_text.empty()
        
        if all_chunks:
            db = FAISS.from_documents(all_chunks, embeddings)
            db.save_local(AppConfig.VECTOR_DB_PATH)
            return db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})
        return None

    # --- CORE: H√ÄM SINH C√ÇU TR·∫¢ L·ªúI & HI·ªÇN TH·ªä NGU·ªíN ---
    @staticmethod
    def generate_response(client, retriever, query) -> Generator[str, None, None]:
        if not retriever:
            yield "D·ªØ li·ªáu ƒëang ƒë∆∞·ª£c t·∫£i, b·∫°n ch·ªù m·ªôt ch√∫t nh√©..."
            return

        # 1. Truy xu·∫•t d·ªØ li·ªáu th√¥
        docs = retriever.invoke(query)
        
        # 2. Ch·∫•m ƒëi·ªÉm ∆∞u ti√™n (SGK > SGV > Code)
        scored_docs = []
        for doc in docs:
            src = doc.metadata.get('source', '')
            score = 0.0
            if "KNTT" in src or "SGK" in src: score = 1.0 # ∆Øu ti√™n cao nh·∫•t
            elif "GV" in src: score = 0.5
            scored_docs.append({"doc": doc, "bonus": score})
        
        # 3. Rerank (S·∫Øp x·∫øp l·∫°i b·∫±ng AI)
        final_docs = []
        try:
            ranker = RAGEngine.load_reranker()
            if ranker and scored_docs:
                passages = [{"id": str(i), "text": x["doc"].page_content, "meta": x["doc"].metadata} for i, x in enumerate(scored_docs)]
                req = RerankRequest(query=query, passages=passages)
                results = ranker.rank(req)
                
                # T√≠nh ƒëi·ªÉm cu·ªëi = ƒêi·ªÉm AI + ƒêi·ªÉm Bonus
                reranked = []
                for res in results:
                    idx = int(res['id'])
                    final_score = res['score'] + (scored_docs[idx]['bonus'] * 0.3)
                    reranked.append({"res": res, "score": final_score})
                
                reranked.sort(key=lambda x: x['score'], reverse=True)
                final_docs = [Document(page_content=r['res']['text'], metadata=r['res']['meta']) for r in reranked[:AppConfig.FINAL_K]]
            else:
                # Fallback n·∫øu kh√¥ng c√≥ Ranker
                scored_docs.sort(key=lambda x: x['bonus'], reverse=True)
                final_docs = [x["doc"] for x in scored_docs[:AppConfig.FINAL_K]]
        except:
            final_docs = [x["doc"] for x in scored_docs[:AppConfig.FINAL_K]]

        if not final_docs:
            yield "Xin l·ªói, hi·ªán t·∫°i trong CSDL SGK ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y."
            return

        # 4. Chu·∫©n b·ªã Context v√† Ngu·ªìn hi·ªÉn th·ªã (Badge)
        context_text = ""
        source_badges_html = ""
        seen_sources = set()

        for doc in final_docs:
            context_text += f"---\nN·ªôi dung: {doc.page_content}\n"
            
            # X·ª≠ l√Ω hi·ªÉn th·ªã Badge
            src_raw = doc.metadata.get('source', 'T√†i li·ªáu')
            lesson = doc.metadata.get('lesson', '').replace('B√†i', 'B.').strip()
            
            # Logic m√†u s·∫Øc
            if "KNTT" in src_raw or "SGK" in src_raw:
                color, icon, lbl = "#0077b6", "üìò", "SGK Tin h·ªçc" # Blue
            elif "GV" in src_raw:
                color, icon, lbl = "#d35400", "üìô", "SGV Tin h·ªçc" # Orange
            elif "Python" in src_raw:
                color, icon, lbl = "#27ae60", "üêç", "Code Python" # Green
            else:
                color, icon, lbl = "#7f8c8d", "üìÑ", "T√†i li·ªáu kh√°c" # Grey
            
            uid = f"{lbl}-{lesson}"
            if uid not in seen_sources:
                source_badges_html += f"""
                <span class="source-badge" style="background-color: {color};">
                    {icon} {lbl} > {lesson}
                </span>
                """
                seen_sources.add(uid)

        # 5. G·ªçi LLM
        sys_prompt = f"""B·∫°n l√† Tr·ª£ l√Ω ·∫£o KTC, chuy√™n gia v·ªÅ Tin h·ªçc THPT (SGK K·∫øt n·ªëi tri th·ª©c).
D·ª±a v√†o ng·ªØ c·∫£nh sau:
{context_text}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa h·ªçc sinh: "{query}"
Y√™u c·∫ßu:
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, s∆∞ ph·∫°m.
- N·∫øu l√† code Python, h√£y gi·∫£i th√≠ch t·ª´ng d√≤ng.
- TUY·ªÜT ƒê·ªêI KH√îNG t·ª± b·ªãa ra ngu·ªìn t√†i li·ªáu.
"""
        try:
            stream = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[{"role": "system", "content": sys_prompt}],
                stream=True,
                temperature=AppConfig.LLM_TEMPERATURE
            )
            
            # Stream n·ªôi dung text tr∆∞·ªõc
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            
            # Cu·ªëi c√πng yield HTML badge ngu·ªìn
            if source_badges_html:
                yield f"\n\n<div style='margin-top:10px; padding-top:10px; border-top:1px dashed #ccc;'>{source_badges_html}</div>"
                
        except Exception as e:
            yield f"ƒêang g·∫∑p s·ª± c·ªë k·∫øt n·ªëi AI: {str(e)}"

# ===================
# 4. MAIN APPLICATION
# ===================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è L·ªói th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.info("G·ª£i √Ω: Ki·ªÉm tra file requirements.txt (c·∫ßn: langchain, groq, flashrank, llama-parse, ...)")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    # Kh·ªüi t·∫°o session
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! M√¨nh l√† KTC Chatbot. B·∫°n c·∫ßn h·ªó tr·ª£ ki·∫øn th·ª©c b√†i n√†o trong SGK?"}]

    groq_client = RAGEngine.load_groq_client()

    # Kh·ªüi t·∫°o Retriever (Ch·∫°y 1 l·∫ßn)
    if "retriever_engine" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông ƒë·ªông c∆° tri th·ª©c s·ªë..."):
            embeddings = RAGEngine.load_embedding_model()
            st.session_state.retriever_engine = RAGEngine.build_hybrid_retriever(embeddings)
    
    # Render l·ªãch s·ª≠ chat
    bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"
    for msg in st.session_state.messages:
        role = msg["role"]
        avatar = "üßë‚Äçüéì" if role == "user" else bot_avatar
        with st.chat_message(role, avatar=avatar):
            st.markdown(msg["content"], unsafe_allow_html=True)

    # X·ª≠ l√Ω input
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar=bot_avatar):
            response_placeholder = st.empty()
            full_response = ""
            
            # G·ªçi Generator
            response_gen = RAGEngine.generate_response(
                groq_client,
                st.session_state.retriever_engine,
                prompt
            )
            
            # Streaming Loop
            for chunk in response_gen:
                full_response += chunk
                response_placeholder.markdown(full_response + "‚ñå", unsafe_allow_html=True)
            
            response_placeholder.markdown(full_response, unsafe_allow_html=True)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()