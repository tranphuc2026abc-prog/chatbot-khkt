import os
import glob
import base64
import streamlit as st
import shutil
import pickle
import re
import hashlib  # <--- TH√äM: ƒê·ªÉ t·∫°o ID c·ªë ƒë·ªãnh (Deterministic ID)
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Generator

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply() # B·∫Øt bu·ªôc cho LlamaParse ch·∫°y trong Streamlit
    from llama_parse import LlamaParse 
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    # Rerank optimization
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="LOGO.jpg",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    LLM_MODEL = 'llama-3.1-8b-instant'

    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"

    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    PROCESSED_MD_DIR = "PROCESSED_MD" 

    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"

    # RAG Parameters
    RETRIEVAL_K = 30       
    FINAL_K = 5            # Gi·∫£m K ƒë·ªÉ gi·∫£m nhi·ªÖu, t·∫≠p trung ƒë·ªô ch√≠nh x√°c
    
    # Hybrid Search Weights
    BM25_WEIGHT = 0.4      
    FAISS_WEIGHT = 0.6     

    LLM_TEMPERATURE = 0.0 # B·∫ÆT BU·ªòC = 0.0 ƒë·ªÉ tri·ªát ti√™u s√°ng t·∫°o ·∫£o gi√°c

# ===============================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER ) 
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown, .stButton, .stTextInput, .stChatInput {
                font-family: 'Inter', sans-serif !important;
            }
            section[data-testid="stSidebar"] {
                background-color: #f8f9fa; border-right: 1px solid #e9ecef;
            }
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                margin-bottom: 5px; text-align: center; text-transform: uppercase;
            }
            .project-sub {
                font-size: 0.8rem; color: #6c757d; text-align: center;
                margin-bottom: 15px; font-style: italic;
            }
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 8px 20px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0;
                font-size: 2.2rem; letter-spacing: -0.5px;
            }
            .header-left p {
                color: #e0fbfc; margin: 5px 0 0 0; font-size: 1rem; opacity: 0.9;
            }
            .header-right img {
                border-radius: 50%; border: 3px solid rgba(255,255,255,0.3);
                box-shadow: 0 4px 10px rgba(0,0,0,0.2); width: 100px; height: 100px;
                object-fit: cover;
            }
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
            [data-testid="stChatMessageContent"]:has(+ [data-testid="stChatMessageAvatar"]) {
                background: #e3f2fd; color: #0d47a1;
            }
            [data-testid="stChatMessageContent"]:not(:has(+ [data-testid="stChatMessageAvatar"])) {
                background: white; border: 1px solid #e9ecef;
                border-left: 5px solid #00b4d8;
            }
            /* Style cho Citation chu·∫©n KHKT */
            .citation-source {
                font-size: 0.75em;
                color: #ffffff; 
                background-color: #d63384; /* M√†u h·ªìng ƒë·∫≠m */
                padding: 2px 8px;
                border-radius: 10px;
                font-weight: 600;
                margin-left: 4px;
                display: inline-block;
                vertical-align: middle;
                cursor: help;
            }
            div.stButton > button {
                border-radius: 8px; background-color: white; color: #0077b6;
                border: 1px solid #90e0ef; transition: all 0.2s;
            }
            div.stButton > button:hover {
                background-color: #0077b6; color: white;
                border-color: #0077b6; box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            }
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div class="project-sub">S·∫£n ph·∫©m d·ª± thi KHKT c·∫•p T·ªânh</div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <div style="display: flex; justify-content: space-between;">
                        <span style="font-weight: 600; color: #555;">T√°c gi·∫£:</span>
                        <span style="text-align: right; color: #222;"><b>B√πi T√° T√πng</b><br><b>Cao S·ªπ B·∫£o Chung</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">GVHD:</span>
                        <span style="text-align: right; color: #222;">Th·∫ßy <b>Nguy·ªÖn Th·∫ø Khanh</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">NƒÉm h·ªçc:</span>
                        <span style="text-align: right; color: #222;"><b>2025 - 2026</b></span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                st.session_state.pop('retriever_engine', None)
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" alt="Logo">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p style="font-size: 1.1rem; margin-top: 5px;">H·ªçc Tin d·ªÖ d√†ng - Thao t√°c v·ªØng v√†ng</p>
            </div>
            <div class="header-right">
                {img_html}
            </div>
        </div>
        """, unsafe_allow_html=True)

# ==================================
# 3. LOGIC BACKEND - VERIFIABLE HYBRID RAG (ƒê√É CHU·∫®N H√ìA KHKT)
# ==================================

class RAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            if not api_key:
                return None
            return Groq(api_key=api_key)
        except Exception:
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        try:
            return HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            st.error(f"L·ªói t·∫£i Embedding: {e}")
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except Exception as e:
            return None

    @staticmethod
    def _structural_chunking(text: str, source_meta: dict) -> List[Document]:
        """
        K·ªπ thu·∫≠t Chunking theo c·∫•u tr√∫c SGK (Ch∆∞∆°ng > B√†i > M·ª•c).
        C·∫¢I TI·∫æN QUAN TR·ªåNG: S·ª≠ d·ª•ng Deterministic ID (MD5 Hash) thay v√¨ UUID ng·∫´u nhi√™n.
        ƒê·∫£m b·∫£o t√≠nh t√°i l·∫≠p (Reproducibility) cho KHKT.
        """
        lines = text.split('\n')
        chunks = []
        
        current_chapter = "Ch∆∞∆°ng m·ªü ƒë·∫ßu"
        current_lesson = "B√†i m·ªü ƒë·∫ßu"
        current_section = "N·ªôi dung"
        
        buffer = []

        # --- REGEX PATTERNS CHO SGK VI·ªÜT NAM ---
        p_chapter = re.compile(r'^#*\s*\**\s*(CH∆Ø∆†NG|Ch∆∞∆°ng)\s+([IVX0-9]+).*$', re.IGNORECASE)
        p_lesson = re.compile(r'^#*\s*\**\s*(B√ÄI|B√†i)\s+([0-9]+).*$', re.IGNORECASE)
        p_section = re.compile(r'^(###\s+|[IV0-9]+\.\s+|[a-z]\)\s+).*')

        def clean_header(text):
            return text.replace('#', '').replace('*', '').strip()

        def commit_chunk(buf, meta):
            if not buf: return
            content = "\n".join(buf).strip()
            if len(content) < 30: return # B·ªè qua chunk qu√° ng·∫Øn r√°c
            
            # --- T·∫†O ID C·ªê ƒê·ªäNH (DETERMINISTIC ID) ---
            # K·∫øt h·ª£p T√™n file + N·ªôi dung ƒë·ªÉ hash. ƒê·∫£m b·∫£o ID kh√¥ng ƒë·ªïi khi ch·∫°y l·∫°i.
            raw_id_str = f"{meta.get('source', '')}_{content}"
            chunk_uid = hashlib.md5(raw_id_str.encode('utf-8')).hexdigest()[:8]
            
            new_meta = meta.copy()
            new_meta.update({
                "chunk_uid": chunk_uid,
                "chapter": current_chapter,
                "lesson": current_lesson,
                "section": current_section,
                "context_str": f"{current_chapter} > {current_lesson}" 
            })
            
            chunks.append(Document(page_content=content, metadata=new_meta))

        for line in lines:
            line_stripped = line.strip()
            if not line_stripped: continue
            
            if p_chapter.match(line_stripped):
                commit_chunk(buffer, source_meta)
                buffer = []
                current_chapter = clean_header(line_stripped)
                current_lesson = "T·ªïng quan ch∆∞∆°ng"
            
            elif p_lesson.match(line_stripped):
                commit_chunk(buffer, source_meta)
                buffer = []
                current_lesson = clean_header(line_stripped)
                
            elif p_section.match(line_stripped) or line_stripped.startswith("### "):
                commit_chunk(buffer, source_meta)
                buffer = []
                current_section = clean_header(line_stripped)
                
            elif line_stripped.startswith("# "): 
                commit_chunk(buffer, source_meta)
                buffer = []
                current_chapter = clean_header(line_stripped)
            elif line_stripped.startswith("## "): 
                commit_chunk(buffer, source_meta)
                buffer = []
                current_lesson = clean_header(line_stripped)
            else:
                buffer.append(line)
        
        commit_chunk(buffer, source_meta)
        return chunks

    @staticmethod
    def _parse_pdf_with_llama(file_path: str) -> str:
        os.makedirs(AppConfig.PROCESSED_MD_DIR, exist_ok=True)
        file_name = os.path.basename(file_path)
        md_file_path = os.path.join(AppConfig.PROCESSED_MD_DIR, f"{file_name}.md")
        
        if os.path.exists(md_file_path):
            with open(md_file_path, "r", encoding="utf-8") as f:
                return f.read()
        
        llama_api_key = st.secrets.get("LLAMA_CLOUD_API_KEY")
        if not llama_api_key:
            return "ERROR: Missing LLAMA_CLOUD_API_KEY"

        try:
            parser = LlamaParse(
                api_key=llama_api_key,
                result_type="markdown",
                language="vi",
                verbose=True,
                parsing_instruction="ƒê√¢y l√† t√†i li·ªáu gi√°o khoa Tin h·ªçc. H√£y gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng b·∫£ng bi·ªÉu, code block v√† c·∫•u tr√∫c ch∆∞∆°ng m·ª•c (#, ##, ###)."
            )
            documents = parser.load_data(file_path)
            markdown_text = documents[0].text
            
            with open(md_file_path, "w", encoding="utf-8") as f:
                f.write(markdown_text)
            
            return markdown_text
        except Exception as e:
            return f"Error parsing {file_name}: {str(e)}"

    @staticmethod
    def _read_and_process_files(pdf_dir: str) -> List[Document]:
        if not os.path.exists(pdf_dir):
            return []
        
        pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
        all_chunks: List[Document] = []
        status_text = st.empty()

        for file_path in pdf_files:
            source_file = os.path.basename(file_path)
            status_text.text(f"ƒêang x·ª≠ l√Ω c·∫•u tr√∫c tri th·ª©c: {source_file}...")
            
            markdown_content = RAGEngine._parse_pdf_with_llama(file_path)
            
            if "ERROR" not in markdown_content and len(markdown_content) > 50:
                 meta = {
                     "source": source_file, 
                 }
                 file_chunks = RAGEngine._structural_chunking(markdown_content, meta)
                 all_chunks.extend(file_chunks)
            else:
                pass 
                
        status_text.empty()
        return all_chunks

    @staticmethod
    def build_hybrid_retriever(embeddings):
        if not embeddings: return None

        vector_db = None
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                vector_db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            except Exception: pass

        if not vector_db:
            chunk_docs = RAGEngine._read_and_process_files(AppConfig.PDF_DIR)
            if not chunk_docs:
                st.error(f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ho·∫∑c l·ªói x·ª≠ l√Ω trong {AppConfig.PDF_DIR}")
                return None
            
            vector_db = FAISS.from_documents(chunk_docs, embeddings)
            vector_db.save_local(AppConfig.VECTOR_DB_PATH)

        try:
            docstore_docs = list(vector_db.docstore._dict.values())
            bm25_retriever = BM25Retriever.from_documents(docstore_docs)
            bm25_retriever.k = AppConfig.RETRIEVAL_K

            faiss_retriever = vector_db.as_retriever(
                search_type="mmr",
                search_kwargs={"k": AppConfig.RETRIEVAL_K, "lambda_mult": 0.5}
            )

            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, faiss_retriever],
                weights=[AppConfig.BM25_WEIGHT, AppConfig.FAISS_WEIGHT]
            )
            return ensemble_retriever
        except Exception:
            return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})

    # =========================================================================
    # STRICT RAG GENERATION LOGIC - PHI√äN B·∫¢N KHKT V2 (AUDITABLE)
    # =========================================================================
    @staticmethod
    def generate_response(client, retriever, query) -> Generator[str, None, None]:
        """
        Quy tr√¨nh x·ª≠ l√Ω nghi√™m ng·∫∑t 3 b∆∞·ªõc:
        1. Retrieval: L·∫•y chunk & ID.
        2. Generation: Y√™u c·∫ßu LLM g·∫Øn ID [ID:xxxx] v√†o cu·ªëi m·ªói √Ω.
        3. Audit (H·∫≠u ki·ªÉm): Parse ID t·ª´ c√¢u tr·∫£ l·ªùi -> ƒê·ªëi chi·∫øu v·ªõi ID g·ªëc.
           - N·∫øu c√≥ ID ma (Hallucination) -> RETURN DEFAULT ERROR.
           - N·∫øu kh√¥ng c√≥ ID n√†o -> RETURN DEFAULT ERROR.
           - Ch·ªâ tr·∫£ v·ªÅ khi ki·ªÉm ch·ª©ng 100%.
        """
        FAIL_MESSAGE = "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong SGK hi·ªán c√≥."

        if not retriever:
            yield "H·ªá th·ªëng ƒëang kh·ªüi t·∫°o..."
            return
        
        # --- B∆Ø·ªöC 1: RETRIEVAL & RERANK ---
        initial_docs = retriever.invoke(query)
        
        # Scoring ∆∞u ti√™n SGK
        scored_docs = []
        for doc in initial_docs:
            src = doc.metadata.get('source', '')
            bonus = 0.0
            if "KNTT" in src: bonus = 2.0 
            elif "_GV_" in src: bonus = 0.5
            scored_docs.append({"doc": doc, "bonus": bonus})
            
        final_docs = []
        try:
            ranker = RAGEngine.load_reranker()
            if ranker and scored_docs:
                passages = [{"id": str(i), "text": d["doc"].page_content, "meta": d["doc"].metadata} for i, d in enumerate(scored_docs)]
                rerank_req = RerankRequest(query=query, passages=passages)
                results = ranker.rank(rerank_req)
                
                # Sort by (Score + Bonus)
                results_scored = []
                for res in results:
                    idx = int(res['id'])
                    total = res['score'] + (scored_docs[idx]['bonus'] * 0.2)
                    results_scored.append((res, total))
                
                results_scored.sort(key=lambda x: x[1], reverse=True)
                top_k = results_scored[:AppConfig.FINAL_K]
                final_docs = [Document(page_content=r[0]['text'], metadata=r[0]['meta']) for r in top_k]
            else:
                scored_docs.sort(key=lambda x: x['bonus'], reverse=True)
                final_docs = [item["doc"] for item in scored_docs][:AppConfig.FINAL_K]
        except:
            final_docs = [item["doc"] for item in scored_docs][:AppConfig.FINAL_K]

        if not final_docs:
            yield FAIL_MESSAGE
            return

        # --- B∆Ø·ªöC 2: CONTEXT PREPARATION ---
        # Map: chunk_uid -> Display Name
        valid_uids_map = {} 
        context_parts = []
        
        for doc in final_docs:
            uid = doc.metadata.get('chunk_uid')
            if not uid: continue # B·ªè qua n·∫øu l·ªói kh√¥ng c√≥ ID
            
            src_raw = doc.metadata.get('source', '').replace('.pdf', '').replace('_', ' ')
            lesson = doc.metadata.get('lesson', '').strip()
            
            # T·∫°o t√™n hi·ªÉn th·ªã ƒë·∫πp cho citation
            if "KNTT" in src_raw: badge = "SGK KNTT"
            elif "GV" in src_raw: badge = "SGV"
            else: badge = "T√†i li·ªáu"
            
            display_name = f"{badge}: {lesson}"
            valid_uids_map[uid] = display_name
            
            # Context input ph·∫£i t∆∞·ªùng minh ID ƒë·ªÉ LLM copy
            context_parts.append(f"SOURCE_ID: {uid}\nCONTENT: {doc.page_content}\n---")

        full_context_str = "\n".join(context_parts)

        # --- B∆Ø·ªöC 3: STRICT PROMPT ---
        system_prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI gi√°o d·ª•c. Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√°c ngu·ªìn ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.

D·ªÆ LI·ªÜU NGU·ªíN (CONTEXT):
{full_context_str}

Y√äU C·∫¶U NGHI√äM NG·∫∂T (VI PH·∫†M S·∫º B·ªä H·ª¶Y B·ªé):
1. M·ªçi √Ω tr·∫£ l·ªùi ƒë·ªÅu ph·∫£i tr√≠ch d·∫´n ngu·ªìn b·∫±ng ID ch√≠nh x√°c.
2. ƒê·ªãnh d·∫°ng tr√≠ch d·∫´n: [ID:xxxxxxxx] (ƒê·∫∑t ·ªü cu·ªëi c√¢u ch·ª©a th√¥ng tin).
3. TUY·ªÜT ƒê·ªêI KH√îNG t·ª± b·ªãa ra ID kh√¥ng c√≥ trong Context.
4. N·∫øu Context kh√¥ng ƒë·ªß tr·∫£ l·ªùi: Ghi "NO_INFO".
5. Kh√¥ng tr·∫£ l·ªùi ki·ªÉu "Theo t√†i li·ªáu...". H√£y tr·∫£ l·ªùi tr·ª±c ti·∫øp ki·∫øn th·ª©c + [ID].

V√≠ d·ª• ƒë√∫ng: Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh b·∫≠c cao [ID:a1b2c3d4].
"""
        
        try:
            # G·ªçi LLM (Non-streaming ƒë·ªÉ ki·ªÉm so√°t ho√†n to√†n ƒë·∫ßu ra)
            completion = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                temperature=0.0,
                stream=False
            )
            raw_response = completion.choices[0].message.content.strip()

            if "NO_INFO" in raw_response:
                yield FAIL_MESSAGE
                return

            # --- B∆Ø·ªöC 4: AUDIT LAYER (H·∫¨U KI·ªÇM) ---
            # T√¨m t·∫•t c·∫£ c√°c th·∫ª [ID:...] trong c√¢u tr·∫£ l·ªùi
            id_pattern = r'\[ID:([a-zA-Z0-9]+)\]'
            found_ids = re.findall(id_pattern, raw_response)
            
            # Logic ki·ªÉm tra:
            # 1. Ph·∫£i c√≥ √≠t nh·∫•t 1 citation.
            # 2. T·∫•t c·∫£ ID t√¨m th·∫•y PH·∫¢I n·∫±m trong valid_uids_map.
            
            if not found_ids:
                # Kh√¥ng c√≥ citation n√†o -> H·ª¶Y
                yield FAIL_MESSAGE
                return
            
            # Ki·ªÉm tra Hallucination (ID ma)
            is_valid = True
            for fid in found_ids:
                if fid not in valid_uids_map:
                    is_valid = False
                    break
            
            if not is_valid:
                # Ph√°t hi·ªán ID b·ªãa -> H·ª¶Y TO√ÄN B·ªò
                yield FAIL_MESSAGE
                return

            # N·∫øu h·ª£p l·ªá -> Render HTML Badge
            processed_response = raw_response
            # D√πng set ƒë·ªÉ tr√°nh l·∫∑p replacement
            unique_found_ids = set(found_ids)
            
            for uid in unique_found_ids:
                if uid in valid_uids_map:
                    source_name = valid_uids_map[uid]
                    html_badge = f"<span class='citation-source' title='Ngu·ªìn: {source_name}'>{source_name}</span>"
                    # Replace to√†n b·ªô [ID:uid] b·∫±ng badge
                    processed_response = processed_response.replace(f"[ID:{uid}]", html_badge)
            
            # Yield k·∫øt qu·∫£ cu·ªëi c√πng
            yield processed_response

        except Exception as e:
            yield f"L·ªói x·ª≠ l√Ω KHKT: {str(e)}"

# ===================
# 4. MAIN APPLICATION
# ===================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! KTC Chatbot s·∫µn s√†ng h·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c SGK Tin h·ªçc."}]

    groq_client = RAGEngine.load_groq_client()

    if "retriever_engine" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c s·ªë (LlamaParse + Semantic Chunking)..."):
            embeddings = RAGEngine.load_embedding_model()
            st.session_state.retriever_engine = RAGEngine.build_hybrid_retriever(embeddings)
            if st.session_state.retriever_engine:
                st.toast("‚úÖ D·ªØ li·ªáu SGK ƒë√£ s·∫µn s√†ng!", icon="üìö")

    for msg in st.session_state.messages:
        bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else bot_avatar
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"], unsafe_allow_html=True) 

    user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi h·ªçc t·∫≠p...")
    
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar=AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"):
            response_placeholder = st.empty()
            
            # G·ªçi generator
            response_gen = RAGEngine.generate_response(
                groq_client,
                st.session_state.retriever_engine,
                user_input
            )

            full_response = ""
            # V√¨ logic m·ªõi buffer to√†n b·ªô r·ªìi m·ªõi yield, v√≤ng l·∫∑p n√†y th·ª±c t·∫ø ch·ªâ ch·∫°y 1 l·∫ßn v·ªõi full text
            # Nh∆∞ng gi·ªØ c·∫•u tr√∫c n√†y ƒë·ªÉ t∆∞∆°ng th√≠ch n·∫øu sau n√†y mu·ªën streaming t·ª´ng ph·∫ßn an to√†n
            for chunk in response_gen:
                full_response += chunk
                response_placeholder.markdown(full_response, unsafe_allow_html=True)

            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()